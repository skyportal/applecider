{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f260f15f-3c77-4971-b1fc-2f1cc92b3de6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Setup & Configuration\n",
    "# =========================\n",
    "import os, gc, math, time, random, json, itertools, copy, warnings\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    fbeta_score,\n",
    "    top_k_accuracy_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "# ---- Device & seeding\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 42, strict: bool = False, workers_deterministic: bool = False):\n",
    "    torch.manual_seed(seed)\n",
    "    if strict:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        torch.use_deterministic_algorithms(workers_deterministic)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def seed_worker(_id):\n",
    "    wseed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(wseed)\n",
    "    random.seed(wseed)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Global Taxonomy\n",
    "# =========================\n",
    "BROAD_CLASSES = [\"SNI\", \"SNII\", \"CV\", \"AGN\", \"TDE\"]\n",
    "ORIG2BROAD = {\n",
    "    \"SN Ia\": \"SNI\",\n",
    "    \"SN Ib\": \"SNI\",\n",
    "    \"SN Ic\": \"SNI\",\n",
    "    \"SN II\": \"SNII\",\n",
    "    \"SN IIP\": \"SNII\",\n",
    "    \"SN IIn\": \"SNII\",\n",
    "    \"SN IIb\": \"SNII\",\n",
    "    \"Cataclysmic\": \"CV\",\n",
    "    \"AGN\": \"AGN\",\n",
    "    \"Tidal Disruption Event\": \"TDE\",\n",
    "}\n",
    "NUM_CLASSES = len(BROAD_CLASSES)\n",
    "BROAD2ID = {c: i for i, c in enumerate(BROAD_CLASSES)}\n",
    "_SUBCLASS_ID2NAME = [\n",
    "    \"SN Ia\",\n",
    "    \"SN Ib\",\n",
    "    \"SN Ic\",\n",
    "    \"SN II\",\n",
    "    \"SN IIP\",\n",
    "    \"SN IIn\",\n",
    "    \"SN IIb\",\n",
    "    \"Cataclysmic\",\n",
    "    \"AGN\",\n",
    "    \"Tidal Disruption Event\",\n",
    "]\n",
    "ID2BROAD_ID = {i: BROAD2ID[ORIG2BROAD[name]] for i, name in enumerate(_SUBCLASS_ID2NAME)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497525c2-e301-4045-8499-bb9e339d25fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Configuration (EDIT HERE)\n",
    "# =========================\n",
    "CFG = SimpleNamespace(\n",
    "    # --- data\n",
    "    output_dir=\"/work/hdd/bcrv/ffontinelenunes/data/AppleCider/photo_events\",\n",
    "    stats_file=\"feature_stats_day100.npz\",\n",
    "    horizon_days=50.0,\n",
    "    batch_size=256,\n",
    "    sampler_balance=True,\n",
    "    num_workers=8,\n",
    "    # --- model\n",
    "    d_model=128,\n",
    "    n_heads=8,\n",
    "    n_layers=4,\n",
    "    dropout=0.40,\n",
    "    max_len=257,\n",
    "    use_time2vec=True,\n",
    "    # --- base optimizer (only used by train loop)\n",
    "    lr=2e-4,\n",
    "    weight_decay=1e-2,\n",
    "    # --- loss/imbalance\n",
    "    focal_gamma=2.0,\n",
    "    # --- augmentation\n",
    "    cut_time_p=None,  # if used, (.25,.25,.25,.25)\n",
    "    p_dropout=0.10,\n",
    "    jitter_scale=0.10,\n",
    "    flux_nu=8,\n",
    "    # --- training schedule\n",
    "    epochs=150,\n",
    "    patience=30,\n",
    "    # --- reproducibility\n",
    "    seed=666,\n",
    "    strict_seed=False,\n",
    "    workers_deterministic=False,\n",
    "    # --- optional toggles\n",
    "    use_pretrained_encoder=True,  # set to true to strart from ME-PT ckpt\n",
    "    pretrained_path=\"mpt_encoder.pt\",\n",
    "    # --- calibration (optional)\n",
    "    calibrate_with_temperature=True,\n",
    "    calib_bins=15,\n",
    "    # --- fine-tuning strategies\n",
    "    # choose: \"full\" or \"head_then_full\"\n",
    "    finetune_strategy=\"full\",\n",
    "    # Strategy A (FULL)\n",
    "    ft_full_lr=2e-4,\n",
    "    ft_full_weight_decay=1e-2,\n",
    "    # Strategy B (HEAD -> FULL)\n",
    "    h2f_head_epochs=5,\n",
    "    h2f_head_lr=1e-2,\n",
    "    h2f_head_weight_decay=1e-2,\n",
    "    h2f_full_encoder_lr=5e-5,\n",
    "    h2f_full_head_lr=5e-4,\n",
    "    h2f_full_weight_decay=1e-2,\n",
    "    # --- cross-validation (optional)\n",
    "    k_folds=2,  # set to None to disable CV\n",
    "    fold_index=0,\n",
    "    # --- hyperparam search (optional)\n",
    "    hps_enabled=True,\n",
    "    hps_max_trials=2,  # very small, just for smoke test\n",
    "    hps_grid=None,\n",
    "    # --- logging (w/ W&B)\n",
    "    wandb_mode=\"online\",  # \"disabled\",to not log externally\n",
    "    wandb_project=\"photometry_example\",\n",
    "    wandb_run_name=None,\n",
    ")\n",
    "\n",
    "seed_everything(CFG.seed, strict=CFG.strict_seed, workers_deterministic=CFG.workers_deterministic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bd8cb-32a1-4f7c-9e3f-3792915c2b48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Dataset, Dataloaders & friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38869bb-2ab3-45ac-9a6c-3338f629760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Data (dataset, manifests, loaders)\n",
    "# =========================\n",
    "_BAND_OH = np.eye(3, dtype=np.float32)\n",
    "\n",
    "\n",
    "def build_event_tensor(arr):\n",
    "    # arr: columns [t, dt, band_idx, logf, logfe]\n",
    "    dt = np.log1p(arr[:, 0])\n",
    "    dt_prev = np.log1p(arr[:, 1])\n",
    "    logf, logfe = arr[:, 3], arr[:, 4]\n",
    "    oh = _BAND_OH[arr[:, 2].astype(np.int64)]\n",
    "    vec4 = np.stack([dt, dt_prev, logf, logfe], 1)\n",
    "    return torch.from_numpy(np.concatenate([vec4, oh], 1))  # (L, 7)\n",
    "\n",
    "\n",
    "class PhotoEventDataset(Dataset):\n",
    "    def __init__(self, manifest_df, horizon=10.0):\n",
    "        self.df = manifest_df.reset_index(drop=True)\n",
    "        self.horizon = horizon\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        raw = np.load(row.filepath)\n",
    "        arr = raw[\"data\"] if isinstance(raw, np.lib.npyio.NpzFile) else raw\n",
    "        if self.horizon:\n",
    "            arr = arr[arr[:, 0] <= self.horizon]\n",
    "        return build_event_tensor(arr.astype(np.float32)), ID2BROAD_ID[int(row.label)]\n",
    "\n",
    "\n",
    "def load_stats(path: Path):\n",
    "    st = np.load(path)\n",
    "    return (torch.from_numpy(st[\"mean\"]), torch.from_numpy(st[\"std\"]))\n",
    "\n",
    "\n",
    "def build_collate(mean, std):\n",
    "    def collate(batch):\n",
    "        seqs, labels = zip(*batch)\n",
    "        lens = [s.size(0) for s in seqs]\n",
    "        pad = pad_sequence(seqs, batch_first=True)  # (B, L, 7)\n",
    "        mask = torch.stack([torch.cat([torch.zeros(l), torch.ones(pad.size(1) - l)]) for l in lens]).bool()\n",
    "        cont = (pad[..., :4] - mean) / (std + 1e-8)\n",
    "        return torch.cat([cont, pad[..., 4:]], -1), torch.tensor(labels), mask  # x, y, pad_mask\n",
    "\n",
    "    return collate\n",
    "\n",
    "\n",
    "def _read_default_manifests(cfg):\n",
    "    out = Path(cfg.output_dir)\n",
    "    train_df = pd.read_csv(out / \"manifest_train.csv\")\n",
    "    val_df = pd.read_csv(out / \"manifest_val.csv\")\n",
    "    test_df = pd.read_csv(out / \"manifest_test.csv\")\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def _build_kfold_manifests(cfg):\n",
    "    base_train, base_val, test_df = _read_default_manifests(cfg)\n",
    "    full = pd.concat([base_train, base_val], ignore_index=True)\n",
    "    y = full.label.map(ID2BROAD_ID).values\n",
    "    skf = StratifiedKFold(n_splits=cfg.k_folds, shuffle=True, random_state=cfg.seed)\n",
    "    tr_idx, va_idx = list(skf.split(np.zeros(len(full)), y))[cfg.fold_index]\n",
    "    train_df = full.iloc[np.sort(tr_idx)].reset_index(drop=True)\n",
    "    val_df = full.iloc[np.sort(va_idx)].reset_index(drop=True)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def _get_manifests(cfg):\n",
    "    return _read_default_manifests(cfg) if not cfg.k_folds else _build_kfold_manifests(cfg)\n",
    "\n",
    "\n",
    "class BalancedAugDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Optional: oversample a minority class (mainly TDE) with light augmentations.\n",
    "    Controlled via CFG.enable_balanced_aug.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, manifest_df, labels, target_per_class, horizon=10.0, p_dropout=0.1, jitter_scale=0.02, flux_nu=4\n",
    "    ):\n",
    "        self.base_ds = PhotoEventDataset(manifest_df, horizon)\n",
    "        self.df = manifest_df.reset_index(drop=True)\n",
    "        self.labels = labels\n",
    "        self.target = target_per_class\n",
    "        self.horizon = horizon\n",
    "        self.p_dropout = p_dropout\n",
    "        self.jitter_scale = jitter_scale\n",
    "        self.flux_nu = flux_nu\n",
    "\n",
    "        self.tde_idx = BROAD2ID[\"TDE\"]\n",
    "        orig_tde_idxs = np.where(labels == self.tde_idx)[0].tolist()\n",
    "        extras = max(0, target_per_class - len(orig_tde_idxs))\n",
    "        self.indices = list(range(len(self.base_ds))) + list(\n",
    "            np.random.choice(orig_tde_idxs, extras, replace=True)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        ds_idx = self.indices[i]\n",
    "        tensor, label = self.base_ds[ds_idx]\n",
    "        if label == self.tde_idx and i >= len(self.base_ds):\n",
    "            row = self.df.iloc[ds_idx]\n",
    "            raw = np.load(row.filepath)\n",
    "            arr = raw[\"data\"] if isinstance(raw, np.lib.npyio.NpzFile) else raw\n",
    "            arr = arr[arr[:, 0] <= self.horizon]\n",
    "            if len(arr) == 0:\n",
    "                arr = arr[:1]\n",
    "            keep = np.random.rand(len(arr)) > self.p_dropout\n",
    "            keep[0] = True\n",
    "            arr = arr[keep]\n",
    "            if len(arr) == 0:\n",
    "                arr = arr[:1]\n",
    "            t0 = arr[:, 0]\n",
    "            ints = np.diff(np.concatenate([[0.0], t0]))\n",
    "            noise = np.random.randn(len(ints)) * (self.jitter_scale * (ints + 1e-6))\n",
    "            ints = np.clip(ints + noise, 0, None)\n",
    "            tnew = np.cumsum(ints)\n",
    "            arr[:, 0] = tnew\n",
    "            arr[:, 1] = np.concatenate([[0.0], ints[:-1]])\n",
    "            from scipy.stats import t as student_t\n",
    "\n",
    "            logf, logfe = arr[:, 3], arr[:, 4]\n",
    "            f, ferr = np.exp(logf), np.exp(logfe)\n",
    "            fnew = student_t(df=CFG.flux_nu, loc=f, scale=0.15 * ferr).rvs()\n",
    "            arr[:, 3] = np.log(np.clip(fnew, 1e-8, None))\n",
    "            tensor = build_event_tensor(arr.astype(np.float32))\n",
    "        return tensor, label\n",
    "\n",
    "\n",
    "def make_loaders(cfg):\n",
    "    mean, std = load_stats(Path(cfg.output_dir) / cfg.stats_file)\n",
    "    collate = build_collate(mean, std)\n",
    "\n",
    "    train_df, val_df, test_df = _get_manifests(cfg)\n",
    "\n",
    "    gen = None\n",
    "    if cfg.strict_seed:\n",
    "        gen = torch.Generator()\n",
    "        gen.manual_seed(cfg.seed)\n",
    "\n",
    "    # Choose dataset and sampling\n",
    "    if getattr(cfg, \"enable_balanced_aug\", False):\n",
    "        orig_lbls = train_df.label.map(ID2BROAD_ID).values\n",
    "        counts = np.bincount(orig_lbls, minlength=NUM_CLASSES)\n",
    "        ref_idx = BROAD2ID[\"SNII\"]\n",
    "        target = int(counts[ref_idx])\n",
    "        train_ds = BalancedAugDataset(\n",
    "            train_df,\n",
    "            orig_lbls,\n",
    "            target,\n",
    "            horizon=cfg.horizon_days,\n",
    "            p_dropout=cfg.p_dropout,\n",
    "            jitter_scale=cfg.jitter_scale,\n",
    "            flux_nu=cfg.flux_nu,\n",
    "        )\n",
    "        all_labels = [orig_lbls[i] for i in getattr(train_ds, \"indices\")]\n",
    "        freqs = Counter(all_labels)\n",
    "        sample_weights = [1.0 / freqs[l] for l in all_labels]\n",
    "        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "        train_ld = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=cfg.batch_size,\n",
    "            sampler=sampler,\n",
    "            num_workers=cfg.num_workers,\n",
    "            collate_fn=collate,\n",
    "            pin_memory=True,\n",
    "            generator=gen,\n",
    "            worker_init_fn=seed_worker if cfg.strict_seed else None,\n",
    "        )\n",
    "    else:\n",
    "        train_ds = PhotoEventDataset(train_df, cfg.horizon_days)\n",
    "        if cfg.sampler_balance:\n",
    "            lbl = train_df.label.map(ID2BROAD_ID).values\n",
    "            weights = 1 / np.sqrt(np.bincount(lbl, minlength=NUM_CLASSES))[lbl]\n",
    "            sampler = WeightedRandomSampler(torch.from_numpy(weights).float(), len(weights), replacement=True)\n",
    "            train_ld = DataLoader(\n",
    "                train_ds,\n",
    "                batch_size=cfg.batch_size,\n",
    "                sampler=sampler,\n",
    "                num_workers=cfg.num_workers,\n",
    "                collate_fn=collate,\n",
    "                pin_memory=True,\n",
    "                generator=gen,\n",
    "                worker_init_fn=seed_worker if cfg.strict_seed else None,\n",
    "            )\n",
    "        else:\n",
    "            train_ld = DataLoader(\n",
    "                train_ds,\n",
    "                batch_size=cfg.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=cfg.num_workers,\n",
    "                collate_fn=collate,\n",
    "                pin_memory=True,\n",
    "                generator=gen,\n",
    "                worker_init_fn=seed_worker if cfg.strict_seed else None,\n",
    "            )\n",
    "\n",
    "    val_ds = PhotoEventDataset(val_df, cfg.horizon_days)\n",
    "    test_ds = PhotoEventDataset(test_df, cfg.horizon_days)\n",
    "\n",
    "    val_ld = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers,\n",
    "        collate_fn=collate,\n",
    "        pin_memory=True,\n",
    "        generator=gen,\n",
    "        worker_init_fn=seed_worker if cfg.strict_seed else None,\n",
    "    )\n",
    "    test_ld = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers,\n",
    "        collate_fn=collate,\n",
    "        pin_memory=True,\n",
    "        generator=gen,\n",
    "        worker_init_fn=seed_worker if cfg.strict_seed else None,\n",
    "    )\n",
    "    return train_ld, val_ld, test_ld, train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731fcb67-6bf8-49d8-9e12-e817c62a635e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Torch model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2df9b-ab6b-4941-a074-17a1b2cbe172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Model\n",
    "# =========================\n",
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.w0 = nn.Parameter(torch.randn(1))\n",
    "        self.b0 = nn.Parameter(torch.zeros(1))\n",
    "        self.w = nn.Parameter(torch.randn(d_model - 1))\n",
    "        self.b = nn.Parameter(torch.zeros(d_model - 1))\n",
    "\n",
    "    def forward(self, t):  # (B, L)\n",
    "        v0 = self.w0 * t + self.b0\n",
    "        vp = torch.sin(t.unsqueeze(-1) * self.w + self.b)\n",
    "        return torch.cat([v0.unsqueeze(-1), vp], dim=-1)\n",
    "\n",
    "\n",
    "class BaselineCLS(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_layers, num_classes, dropout, max_len=None, use_time2vec=True):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(7, d_model)\n",
    "        self.cls_tok = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.use_time2vec = use_time2vec\n",
    "        self.time2vec = Time2Vec(d_model) if use_time2vec else None\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model, n_heads, d_model * 4, dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, n_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, pad_mask):\n",
    "        B, L, _ = x.shape\n",
    "        h = self.in_proj(x)\n",
    "        if self.use_time2vec:\n",
    "            t = x[..., 0]\n",
    "            te = self.time2vec(t)\n",
    "            h = h + te\n",
    "        tok = self.cls_tok.expand(B, 1, -1)\n",
    "        h = torch.cat([tok, h], dim=1)\n",
    "        pad = torch.cat([torch.zeros(B, 1, device=pad_mask.device, dtype=torch.bool), pad_mask], dim=1)\n",
    "        z = self.encoder(h, src_key_padding_mask=pad)\n",
    "        return self.head(self.norm(z[:, 0]))\n",
    "\n",
    "\n",
    "def load_pretrained_encoder_(model: nn.Module, path: str):\n",
    "    pre = torch.load(path, map_location=device)\n",
    "    sd = model.state_dict()\n",
    "    loaded = 0\n",
    "    for k, v in pre.items():\n",
    "        if k.startswith(\"head.\"):  # skip classifier head\n",
    "            continue\n",
    "        if k in sd and sd[k].shape == v.shape:\n",
    "            sd[k] = v\n",
    "            loaded += 1\n",
    "    model.load_state_dict(sd)\n",
    "    print(f\"Loaded {loaded} pretrained tensors from {path} (encoder only).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9342bd3-5411-40ea-acf3-a775b3d68f6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc1d2c-e2ca-4fa0-9228-5f7be9930d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Plotting helpers\n",
    "# =========================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_loss_curves(curves, title=\"Training curves\", dpi=300):\n",
    "    ep = np.arange(1, len(curves[\"val_auprc\"]) + 1)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(11, 4))\n",
    "    ax[0].plot(ep, curves[\"train_loss\"], label=\"train/loss\")\n",
    "    ax[0].plot(ep, curves[\"val_loss\"], label=\"val/loss\")\n",
    "    ax[0].set(xlabel=\"epoch\", ylabel=\"loss\", title=\"Loss\")\n",
    "    ax[0].legend(frameon=False)\n",
    "    ax[1].plot(ep, curves[\"val_auprc\"], label=\"val/auprc\")\n",
    "    ax[1].plot(ep, curves[\"val_acc\"], label=\"val/acc\")\n",
    "    ax[1].set(xlabel=\"epoch\", ylabel=\"score\", title=\"Validation\")\n",
    "    ax[1].set_ylim(0, 1.02)\n",
    "    ax[1].legend(frameon=False)\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_confusion(y_true, y_pred, title=\"Confusion (norm.)\"):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(NUM_CLASSES))\n",
    "    cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm[np.isnan(cm_norm)] = 0\n",
    "    fig, ax = plt.subplots(figsize=(5.5, 4.5), dpi=300)\n",
    "    sns.heatmap(\n",
    "        cm_norm,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"Purples\",\n",
    "        xticklabels=BROAD_CLASSES,\n",
    "        yticklabels=BROAD_CLASSES,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Pred\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_roc_pr(y_true, probs, title_prefix=\"Test\"):\n",
    "    y_oh = label_binarize(y_true, classes=np.arange(NUM_CLASSES))\n",
    "    # ROC\n",
    "    fig1, ax1 = plt.subplots(figsize=(6, 5), dpi=300)\n",
    "    for i, cls in enumerate(BROAD_CLASSES):\n",
    "        fpr, tpr, _ = roc_curve(y_oh[:, i], probs[:, i])\n",
    "        auc_i = auc(fpr, tpr)\n",
    "        ax1.plot(fpr, tpr, label=f\"{cls} (AUC={auc_i:.2f})\")\n",
    "    fpr_micro, tpr_micro, _ = roc_curve(y_oh.ravel(), probs.ravel())\n",
    "    auc_micro = auc(fpr_micro, tpr_micro)\n",
    "    ax1.plot(fpr_micro, tpr_micro, \"k--\", lw=2.2, label=f\"Micro (AUC={auc_micro:.2f})\")\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", alpha=0.6)\n",
    "    ax1.set(xlabel=\"FPR\", ylabel=\"TPR\", title=f\"{title_prefix} ROC\")\n",
    "    ax1.legend(frameon=False, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig1)\n",
    "    # PR\n",
    "    fig2, ax2 = plt.subplots(figsize=(6, 5), dpi=300)\n",
    "    for i, cls in enumerate(BROAD_CLASSES):\n",
    "        prec, rec, _ = precision_recall_curve(y_oh[:, i], probs[:, i])\n",
    "        ap_i = average_precision_score(y_oh[:, i], probs[:, i])\n",
    "        ax2.step(rec, prec, where=\"post\", label=f\"{cls} (AP={ap_i:.2f})\")\n",
    "    ap_micro = average_precision_score(y_oh, probs, average=\"micro\")\n",
    "    ax2.set(xlabel=\"Recall\", ylabel=\"Precision\", title=f\"{title_prefix} PR (micro AP={ap_micro:.2f})\")\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1.02)\n",
    "    ax2.legend(frameon=False, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig2)\n",
    "\n",
    "\n",
    "def plot_reliability_diagram(bin_contrib, title=\"Reliability\"):\n",
    "    xs, ys = [], []\n",
    "    for lo, hi, prop, acc_bin, conf_bin in bin_contrib:\n",
    "        xs.append(conf_bin)\n",
    "        ys.append(acc_bin)\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4.5))\n",
    "    ax.plot([0, 1], [0, 1], \"--\", color=\"gray\", lw=1)\n",
    "    ax.scatter(xs, ys, s=40)\n",
    "    ax.set(xlabel=\"Confidence\", ylabel=\"Accuracy\", title=title, xlim=(0, 1), ylim=(0, 1))\n",
    "    ax.grid(alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb470e2-50e4-4122-b895-f6925c11d38a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Evaluation helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e385fa6-0339-4bef-9303-1373d1a7ef42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Evaluation helpers\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def collect_logits(model, loader):\n",
    "    ys, logits = [], []\n",
    "    model.eval()\n",
    "    for x, y, m in loader:\n",
    "        x, m = x.to(device), m.to(device)\n",
    "        out = model(x, m)\n",
    "        ys.append(y.numpy())\n",
    "        logits.append(out.detach().cpu())\n",
    "    y = np.concatenate(ys)\n",
    "    L = torch.cat(logits, 0)\n",
    "    return y, L\n",
    "\n",
    "\n",
    "def summarize_on_loader(model, loader):\n",
    "    y, L = collect_logits(model, loader)\n",
    "    P = torch.softmax(L, 1).numpy()\n",
    "    y_oh = label_binarize(y, classes=np.arange(NUM_CLASSES))\n",
    "    metrics = {\n",
    "        \"acc\": float((P.argmax(1) == y).mean()),\n",
    "        \"auprc\": float(average_precision_score(y_oh, P, average=\"macro\")),\n",
    "        \"top2\": float(top_k_accuracy_score(y, P, k=2)),\n",
    "        \"top3\": float(top_k_accuracy_score(y, P, k=3)),\n",
    "    }\n",
    "    return metrics, (y, P, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba4870e-4558-4542-aa97-35adbbbfc619",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ea39d2-db8c-4243-bd73-00d3ab1e5dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Training\n",
    "# =========================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self, gamma: float = 2.0, alpha: torch.Tensor = None, eps: float = 0.1, reduction: str = \"mean\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gamma, self.alpha, self.eps, self.reduction = gamma, alpha, eps, reduction\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, target: torch.Tensor):\n",
    "        B, C = logits.shape\n",
    "        logp = F.log_softmax(logits, dim=1)\n",
    "        p = logp.exp()\n",
    "        if self.eps > 0:\n",
    "            smooth = torch.full_like(logp, fill_value=self.eps / (C - 1))\n",
    "            smooth.scatter_(1, target.unsqueeze(1), 1.0 - self.eps)\n",
    "            y = smooth\n",
    "        else:\n",
    "            y = F.one_hot(target, num_classes=C).float()\n",
    "        focal_weight = (1.0 - p).pow(self.gamma)\n",
    "        if self.alpha is not None:\n",
    "            focal_weight = focal_weight * self.alpha.view(1, C)\n",
    "        loss = -(y * focal_weight * logp).sum(dim=1)\n",
    "        return loss.mean() if self.reduction == \"mean\" else loss.sum()\n",
    "\n",
    "\n",
    "def _cut_time(mask, dt_first, probs):\n",
    "    B, _ = mask.shape\n",
    "    horizons = np.random.choice([1, 3, 5, 10], size=B, p=probs)\n",
    "    for b, h in enumerate(horizons):\n",
    "        mask[b, dt_first[b] > h] = True\n",
    "\n",
    "\n",
    "def run_epoch(model, loader, optimizer, criterion, is_train, cfg: SimpleNamespace):\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    tot_loss, tot_correct, tot_N = 0.0, 0, 0\n",
    "    y_true_list, y_prob_list = [], []\n",
    "\n",
    "    for x, y, m in loader:\n",
    "        x, y, m = x.to(device), y.to(device), m.to(device)\n",
    "        if is_train and cfg.cut_time_p is not None:\n",
    "            dt_first = x[..., 0].exp() - 1\n",
    "            _cut_time(m, dt_first, cfg.cut_time_p)\n",
    "\n",
    "        logits = model(x, m)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        tot_loss += loss.item() * y.size(0)\n",
    "        tot_correct += (logits.argmax(1) == y).sum().item()\n",
    "        tot_N += y.size(0)\n",
    "\n",
    "        y_true_list.append(y.detach().cpu().numpy())\n",
    "        y_prob_list.append(torch.softmax(logits, 1).detach().cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true_list)\n",
    "    y_prob = np.concatenate(y_prob_list)\n",
    "    y_true_oh = label_binarize(y_true, classes=np.arange(NUM_CLASSES))\n",
    "    auprc_macro = average_precision_score(y_true_oh, y_prob, average=\"macro\")\n",
    "    return tot_loss / tot_N, tot_correct / tot_N, auprc_macro\n",
    "\n",
    "\n",
    "def build_model_and_criterion(cfg, train_df):\n",
    "    model = BaselineCLS(\n",
    "        cfg.d_model,\n",
    "        cfg.n_heads,\n",
    "        cfg.n_layers,\n",
    "        NUM_CLASSES,\n",
    "        cfg.dropout,\n",
    "        cfg.max_len,\n",
    "        use_time2vec=getattr(cfg, \"use_time2vec\", True),\n",
    "    ).to(device)\n",
    "    if cfg.use_pretrained_encoder:\n",
    "        load_pretrained_encoder_(model, cfg.pretrained_path)\n",
    "    cnts = torch.bincount(torch.tensor(train_df.label.map(ID2BROAD_ID).values), minlength=NUM_CLASSES)\n",
    "    alpha = (1 / torch.sqrt(cnts + 1e-6)).to(device)\n",
    "    crit = FocalLoss(cfg.focal_gamma, alpha)\n",
    "    return model, crit\n",
    "\n",
    "\n",
    "def _train_epochs(model, crit, train_ld, val_ld, optimizer, cfg, tag=\"fit\"):\n",
    "    best_auprc, best_state, no_improve = -1.0, None, 0\n",
    "    curves = {\"train_loss\": [], \"val_loss\": [], \"val_auprc\": [], \"val_acc\": []}\n",
    "    for ep in range(1, cfg.epochs + 1):\n",
    "        tr_loss, tr_acc, tr_auprc = run_epoch(model, train_ld, optimizer, crit, True, cfg)\n",
    "        va_loss, va_acc, va_auprc = run_epoch(model, val_ld, None, crit, False, cfg)\n",
    "        curves[\"train_loss\"].append(tr_loss)\n",
    "        curves[\"val_loss\"].append(va_loss)\n",
    "        curves[\"val_auprc\"].append(va_auprc)\n",
    "        curves[\"val_acc\"].append(va_acc)\n",
    "        if va_auprc > best_auprc + 1e-4:\n",
    "            best_auprc = va_auprc\n",
    "            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= cfg.patience:\n",
    "                print(f\"[{tag}] Early stop @ epoch {ep} | best val AUPRC={best_auprc:.4f}\")\n",
    "                break\n",
    "        print(\n",
    "            f\"[{tag}] EP{ep:03d} | tl {tr_loss:.3f} | vl {va_loss:.3f} | vAUPRC {va_auprc:.3f} | vAcc {va_acc:.3f}\"\n",
    "        )\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    return model, curves, best_auprc\n",
    "\n",
    "\n",
    "def run_finetune_full(cfg):\n",
    "    train_ld, val_ld, test_ld, train_df, val_df, test_df = make_loaders(cfg)\n",
    "    model, crit = build_model_and_criterion(cfg, train_df)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=cfg.ft_full_lr, weight_decay=cfg.ft_full_weight_decay\n",
    "    )\n",
    "    model, curves, best_val = _train_epochs(model, crit, train_ld, val_ld, optimizer, cfg, tag=\"ft_full\")\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"loaders\": (train_ld, val_ld, test_ld),\n",
    "        \"curves\": curves,\n",
    "        \"best_val_auprc\": best_val,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_finetune_head_then_full(cfg):\n",
    "    train_ld, val_ld, test_ld, train_df, val_df, test_df = make_loaders(cfg)\n",
    "    model, crit = build_model_and_criterion(cfg, train_df)\n",
    "    # Stage 1: head-only\n",
    "    for n, p in model.named_parameters():\n",
    "        if not n.startswith(\"head.\"):\n",
    "            p.requires_grad = False\n",
    "    head_opt = torch.optim.AdamW(\n",
    "        model.head.parameters(), lr=cfg.h2f_head_lr, weight_decay=cfg.h2f_head_weight_decay\n",
    "    )\n",
    "    for ep in range(1, cfg.h2f_head_epochs + 1):\n",
    "        tr_loss, tr_acc, tr_auprc = run_epoch(model, train_ld, head_opt, crit, True, cfg)\n",
    "        va_loss, va_acc, va_auprc = run_epoch(model, val_ld, None, crit, False, cfg)\n",
    "        print(f\"[HEAD] EP{ep:02d} | tl {tr_loss:.3f} | vl {va_loss:.3f} | vAUPRC {va_auprc:.3f}\")\n",
    "    # Stage 2: unfreeze all and train\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "    full_opt = torch.optim.AdamW(\n",
    "        [\n",
    "            {\"params\": model.encoder.parameters(), \"lr\": cfg.h2f_full_encoder_lr},\n",
    "            {\"params\": model.head.parameters(), \"lr\": cfg.h2f_full_head_lr},\n",
    "            {\"params\": model.in_proj.parameters(), \"lr\": cfg.h2f_full_encoder_lr},\n",
    "            {\"params\": model.norm.parameters(), \"lr\": cfg.h2f_full_encoder_lr},\n",
    "            *(\n",
    "                ([{\"params\": model.time2vec.parameters(), \"lr\": cfg.h2f_full_encoder_lr}])\n",
    "                if getattr(model, \"time2vec\", None) is not None\n",
    "                else []\n",
    "            ),\n",
    "        ],\n",
    "        weight_decay=cfg.h2f_full_weight_decay,\n",
    "    )\n",
    "    model, curves, best_val = _train_epochs(model, crit, train_ld, val_ld, full_opt, cfg, tag=\"h2f_full\")\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"loaders\": (train_ld, val_ld, test_ld),\n",
    "        \"curves\": curves,\n",
    "        \"best_val_auprc\": best_val,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_finetune(cfg):\n",
    "    return (\n",
    "        run_finetune_head_then_full(cfg)\n",
    "        if cfg.finetune_strategy == \"head_then_full\"\n",
    "        else run_finetune_full(cfg)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360672a6-01ca-494a-b569-216105cf1234",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# (optional) Model calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766d1a4-8e44-486b-b169-2b6f9941813b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Calibration wth temperature scaling. helpers + driver\n",
    "# =========================\n",
    "class TemperatureScaler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log_temp = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, logits):\n",
    "        return logits / self.log_temp.exp()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def temperature(self):\n",
    "        return float(self.log_temp.exp().cpu())\n",
    "\n",
    "\n",
    "def fit_temperature_on_logits(logits: torch.Tensor, labels: torch.Tensor, max_iter=1000):\n",
    "    scaler = TemperatureScaler().to(logits.device)\n",
    "    nll = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.LBFGS([scaler.log_temp], lr=0.25, max_iter=50, line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = nll(scaler(logits), labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        _ = optimizer.step(closure)\n",
    "        break\n",
    "    return scaler\n",
    "\n",
    "\n",
    "def expected_calibration_error(probs, labels, n_bins=15):\n",
    "    confidences = probs.max(1)\n",
    "    predictions = probs.argmax(1)\n",
    "    accuracies = (predictions == labels).astype(float)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece, brier = (\n",
    "        0.0,\n",
    "        np.mean(np.sum((probs - label_binarize(labels, classes=np.arange(probs.shape[1]))) ** 2, axis=1)),\n",
    "    )\n",
    "    bin_contrib = []\n",
    "    for i in range(n_bins):\n",
    "        in_bin = (confidences > bins[i]) & (confidences <= bins[i + 1])\n",
    "        prop = in_bin.mean()\n",
    "        if prop > 0:\n",
    "            acc_bin = accuracies[in_bin].mean()\n",
    "            conf_bin = confidences[in_bin].mean()\n",
    "            ece += prop * abs(acc_bin - conf_bin)\n",
    "            bin_contrib.append((bins[i], bins[i + 1], prop, acc_bin, conf_bin))\n",
    "    return float(ece), float(brier), bin_contrib\n",
    "\n",
    "\n",
    "def calibrate_if_enabled(cfg, model, loaders):\n",
    "    \"\"\"\n",
    "    Returns None if disabled; else dict with:\n",
    "      - 'T': temperature\n",
    "      - 'val': {'ece_unc','brier_unc','bins_unc', 'ece_cal','brier_cal','bins_cal'}\n",
    "      - 'test': {'acc_unc','auprc_unc','ece_unc','brier_unc', 'acc_cal','auprc_cal','ece_cal','brier_cal'}\n",
    "    \"\"\"\n",
    "    if not cfg.calibrate_with_temperature:\n",
    "        return None\n",
    "    _, val_ld, test_ld = loaders\n",
    "\n",
    "    # Fit on VAL\n",
    "    y_v, L_v = collect_logits(model, val_ld)\n",
    "    scaler = fit_temperature_on_logits(L_v.to(device), torch.tensor(y_v, device=device))\n",
    "    T = scaler.temperature()\n",
    "\n",
    "    # VAL pre/post stats\n",
    "    P_v_uncal = torch.softmax(L_v, 1).numpy()\n",
    "    ece_v_unc, brier_v_unc, bins_unc = expected_calibration_error(P_v_uncal, y_v, n_bins=cfg.calib_bins)\n",
    "    with torch.no_grad():\n",
    "        L_v_cal = scaler(L_v.to(device)).cpu()\n",
    "    P_v_cal = torch.softmax(L_v_cal, 1).numpy()\n",
    "    ece_v_cal, brier_v_cal, bins_cal = expected_calibration_error(P_v_cal, y_v, n_bins=cfg.calib_bins)\n",
    "\n",
    "    # TEST pre/post stats\n",
    "    y_t, L_t = collect_logits(model, test_ld)\n",
    "    y_oh_t = label_binarize(y_t, classes=np.arange(NUM_CLASSES))\n",
    "    P_t_unc = torch.softmax(L_t, 1).numpy()\n",
    "    acc_unc = float((P_t_unc.argmax(1) == y_t).mean())\n",
    "    auprc_unc = float(average_precision_score(y_oh_t, P_t_unc, average=\"macro\"))\n",
    "    with torch.no_grad():\n",
    "        L_t_cal = scaler(L_t.to(device)).cpu()\n",
    "    P_t_cal = torch.softmax(L_t_cal, 1).numpy()\n",
    "    acc_cal = float((P_t_cal.argmax(1) == y_t).mean())\n",
    "    auprc_cal = float(average_precision_score(y_oh_t, P_t_cal, average=\"macro\"))\n",
    "    ece_t_unc, brier_t_unc, bins_t_unc = expected_calibration_error(P_t_unc, y_t, n_bins=cfg.calib_bins)\n",
    "    ece_t_cal, brier_t_cal, bins_t_cal = expected_calibration_error(P_t_cal, y_t, n_bins=cfg.calib_bins)\n",
    "\n",
    "    return dict(\n",
    "        T=T,\n",
    "        val=dict(\n",
    "            ece_unc=ece_v_unc,\n",
    "            brier_unc=brier_v_unc,\n",
    "            bins_unc=bins_unc,\n",
    "            ece_cal=ece_v_cal,\n",
    "            brier_cal=brier_v_cal,\n",
    "            bins_cal=bins_cal,\n",
    "        ),\n",
    "        test=dict(\n",
    "            acc_unc=acc_unc,\n",
    "            auprc_unc=auprc_unc,\n",
    "            ece_unc=ece_t_unc,\n",
    "            brier_unc=brier_t_unc,\n",
    "            acc_cal=acc_cal,\n",
    "            auprc_cal=auprc_cal,\n",
    "            ece_cal=ece_t_cal,\n",
    "            brier_cal=brier_t_cal,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09fd6a-d66e-4ebf-8a7a-cdf6b8143ad1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# (optional) Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754820f-7054-4f0f-a14b-054c2f01864c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Tiny Hyper-parameter Search with W&B Sweeps\n",
    "# =========================\n",
    "# Returns: (best_cfg, best_out), results\n",
    "\n",
    "# Silence the notebook-name warning (this path doesn't need to exist)\n",
    "os.environ.setdefault(\"WANDB_NOTEBOOK_NAME\", \"photometry_example_testing.ipynb\")\n",
    "\n",
    "\n",
    "def tiny_hparam_search(cfg, grid=None, max_trials=3, seed=None):\n",
    "    if getattr(cfg, \"wandb_mode\", None) != \"online\":\n",
    "        raise RuntimeError(\n",
    "            \"Set CFG.wandb_mode='online' and run `wandb login` before calling tiny_hparam_search().\"\n",
    "        )\n",
    "\n",
    "    def _wb_param_spec(name, default):\n",
    "        if isinstance(grid, dict) and name in grid:\n",
    "            spec = grid[name]\n",
    "            if (\n",
    "                isinstance(spec, (list, tuple))\n",
    "                and len(spec) == 2\n",
    "                and all(isinstance(x, (int, float)) for x in spec)\n",
    "            ):\n",
    "                lo, hi = float(spec[0]), float(spec[1])\n",
    "                if name == \"lr\":\n",
    "                    return {\"distribution\": \"log_uniform_values\", \"min\": lo, \"max\": hi}\n",
    "                return {\"distribution\": \"uniform\", \"min\": lo, \"max\": hi}\n",
    "            if isinstance(spec, (list, tuple)) and len(spec) > 0:\n",
    "                return {\"values\": list(spec)}\n",
    "        if name == \"lr\":\n",
    "            return {\"distribution\": \"log_uniform_values\", \"min\": 1e-5, \"max\": 5e-3}\n",
    "        if name == \"dropout\":\n",
    "            return {\"distribution\": \"uniform\", \"min\": 0.0, \"max\": 0.5}\n",
    "        if name == \"d_model\":\n",
    "            n_heads = getattr(cfg, \"n_heads\", 8)\n",
    "            return {\"values\": [d for d in [96, 128, 160, 192] if d % n_heads == 0]}\n",
    "        if name == \"n_layers\":\n",
    "            return {\"values\": [2, 3, 4, 5]}\n",
    "        return {\"values\": [default]}\n",
    "\n",
    "    sweep_config = {\n",
    "        \"name\": \"tiny-bayes-hparam-search\",\n",
    "        \"method\": \"bayes\",\n",
    "        \"metric\": {\"name\": \"best_val_auprc\", \"goal\": \"maximize\"},\n",
    "        \"parameters\": {\n",
    "            \"lr\": _wb_param_spec(\"lr\", cfg.lr),\n",
    "            \"dropout\": _wb_param_spec(\"dropout\", cfg.dropout),\n",
    "            \"d_model\": _wb_param_spec(\"d_model\", cfg.d_model),\n",
    "            \"n_layers\": _wb_param_spec(\"n_layers\", cfg.n_layers),\n",
    "        },\n",
    "        \"early_terminate\": {\"type\": \"hyperband\", \"min_iter\": 2},\n",
    "        **({\"seed\": int(seed)} if seed is not None else {}),\n",
    "    }\n",
    "\n",
    "    base_epochs, base_patience = cfg.epochs, cfg.patience\n",
    "    results = []\n",
    "    best_holder = {\"cfg\": None, \"out\": None, \"score\": -1.0}\n",
    "\n",
    "    def _apply_trial_to_cfg(cfg_base, params: dict):\n",
    "        cfg_i = copy.deepcopy(cfg_base)\n",
    "        cfg_i.epochs = max(3, min(8, base_epochs // 5))  # short schedule\n",
    "        cfg_i.patience = max(2, min(4, base_patience // 3))\n",
    "        cfg_i.lr = float(params.get(\"lr\", cfg.lr))\n",
    "        cfg_i.dropout = float(params.get(\"dropout\", cfg.dropout))\n",
    "        cfg_i.d_model = int(params.get(\"d_model\", cfg.d_model))\n",
    "        cfg_i.n_layers = int(params.get(\"n_layers\", cfg.n_layers))\n",
    "        return cfg_i\n",
    "\n",
    "    def _record_trial(cfg_i, out, score):\n",
    "        nonlocal best_holder, results\n",
    "        score = float(score)\n",
    "        results.append((cfg_i, None, score))\n",
    "        if score > best_holder[\"score\"]:\n",
    "            if best_holder[\"out\"] is not None and \"model\" in best_holder[\"out\"]:\n",
    "                try:\n",
    "                    del best_holder[\"out\"][\"model\"]\n",
    "                except:\n",
    "                    pass\n",
    "            best_holder = {\"cfg\": cfg_i, \"out\": out, \"score\": score}\n",
    "        else:\n",
    "            try:\n",
    "                del out[\"model\"]\n",
    "            except:\n",
    "                pass\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    def _wb_objective():\n",
    "        # the agent uses the sweep project\n",
    "        run = wandb.init(\n",
    "            mode=getattr(cfg, \"wandb_mode\", \"online\"), name=getattr(cfg, \"wandb_run_name\", None), config={}\n",
    "        )\n",
    "        try:\n",
    "            params = dict(wandb.config)\n",
    "            cfg_i = _apply_trial_to_cfg(cfg, params)\n",
    "            print(\n",
    "                f\"\\n[W&B] Trial params: lr={cfg_i.lr:.2e}, dropout={cfg_i.dropout:.2f}, \"\n",
    "                f\"d_model={cfg_i.d_model}, n_layers={cfg_i.n_layers}\"\n",
    "            )\n",
    "\n",
    "            out = run_finetune(cfg_i)\n",
    "\n",
    "            # Log full curves as time-series\n",
    "            curves = out[\"curves\"]\n",
    "            for ep, (tl, vl, va, vacc) in enumerate(\n",
    "                zip(curves[\"train_loss\"], curves[\"val_loss\"], curves[\"val_auprc\"], curves[\"val_acc\"]), start=1\n",
    "            ):\n",
    "                wandb.log(\n",
    "                    {\"epoch\": ep, \"train/loss\": tl, \"val/loss\": vl, \"val/auprc\": va, \"val/acc\": vacc}, step=ep\n",
    "                )\n",
    "\n",
    "            best_ep = int(np.argmax(curves[\"val_auprc\"])) + 1\n",
    "            best_val = float(curves[\"val_auprc\"][best_ep - 1])\n",
    "            best_acc = float(curves[\"val_acc\"][best_ep - 1])\n",
    "            wandb.summary.update(\n",
    "                {\"best_epoch\": best_ep, \"best_val_auprc\": best_val, \"best_val_acc\": best_acc}\n",
    "            )\n",
    "\n",
    "            _record_trial(cfg_i, out, best_val)\n",
    "        finally:\n",
    "            run.finish()\n",
    "\n",
    "    sweep_id = wandb.sweep(\n",
    "        sweep=sweep_config,\n",
    "        project=getattr(cfg, \"wandb_project\", \"sweeps\"),\n",
    "        entity=getattr(cfg, \"wandb_entity\", None),\n",
    "    )\n",
    "    wandb.agent(sweep_id, function=_wb_objective, count=int(max_trials))\n",
    "\n",
    "    # fail loudly so code doesn't crash cryptically\n",
    "    if best_holder[\"out\"] is None:\n",
    "        raise RuntimeError(\"All W&B sweep trials failed. Check the sweep run pages for stack traces.\")\n",
    "\n",
    "    print(f\"\\nW&B sweep — best val AUPRC={best_holder['score']:.4f}\")\n",
    "    return (best_holder[\"cfg\"], best_holder[\"out\"]), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c25378-389b-471f-9968-4944f3317988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cross-validation\n",
    "# =========================\n",
    "def run_crossval_finetune(cfg):\n",
    "    \"\"\"\n",
    "    Runs K folds; returns a DataFrame-like dict and per-fold artifacts.\n",
    "    Each fold calls run_finetune() with cfg.fold_index set.\n",
    "    \"\"\"\n",
    "    assert cfg.k_folds and cfg.k_folds >= 2, \"Set CFG.k_folds >= 2\"\n",
    "    fold_summaries, fold_artifacts = [], []\n",
    "    for fi in range(cfg.k_folds):\n",
    "        cfg_f = copy.deepcopy(cfg)\n",
    "        cfg_f.fold_index = fi\n",
    "        print(f\"\\n===== Fold {fi+1}/{cfg.k_folds} ({cfg.finetune_strategy}) =====\")\n",
    "        out = run_finetune(cfg_f)\n",
    "        # summarize on test with evaluation helpers\n",
    "        _, _, test_ld = out[\"loaders\"]\n",
    "        m, (y, P, L) = summarize_on_loader(out[\"model\"], test_ld)\n",
    "        fold_summaries.append({\"fold\": fi, **m, \"best_val_auprc\": out[\"best_val_auprc\"]})\n",
    "        fold_artifacts.append({\"fold\": fi, \"y\": y, \"P\": P, \"curves\": out[\"curves\"]})\n",
    "    df = pd.DataFrame(fold_summaries)\n",
    "    print(\"\\nCV summary per fold:\")\n",
    "    display(df)\n",
    "    print(\"\\nCV mean ± std:\")\n",
    "    display(df.describe().loc[[\"mean\", \"std\"]])\n",
    "    return df, fold_artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e65e7d-f6fd-40dd-ae46-088da6afe026",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# (optional) Masked Event Reconstruction pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d8221-cd5c-42b9-b9b2-ea9ac27a3c52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Optional: ME-MPT pretraining\n",
    "# =========================\n",
    "MASK_P, PT_EPOCHS, PT_BATCH, PT_LR = 0.30, 50, 256, 5e-4\n",
    "LAMBDA_F, LAMBDA_B, LAMBDA_DT = 5.0, 3.0, 5.0\n",
    "\n",
    "\n",
    "def _mask_batch(x, pad_mask):\n",
    "    masked = torch.zeros_like(pad_mask)\n",
    "    B, L, _ = x.shape\n",
    "    for b in range(B):\n",
    "        valid = (~pad_mask[b]).nonzero(as_tuple=True)[0]\n",
    "        k = max(int(len(valid) * MASK_P), 3)\n",
    "        num_each = k // 3\n",
    "        extras = k - 3 * num_each\n",
    "        bands = x[b, :, 4:7].argmax(-1)\n",
    "        idxs = []\n",
    "        for band in [0, 1, 2]:\n",
    "            valid_b = valid[bands[valid] == band]\n",
    "            if len(valid_b) > 0:\n",
    "                take = min(len(valid_b), num_each)\n",
    "                perm = torch.randperm(len(valid_b))[:take]\n",
    "                idxs.append(valid_b[perm])\n",
    "        if extras > 0:\n",
    "            remaining = (\n",
    "                torch.cat(idxs) if len(idxs) > 0 else torch.tensor([], device=valid.device, dtype=valid.dtype)\n",
    "            )\n",
    "            pool = valid[~torch.isin(valid, remaining)]\n",
    "            if len(pool) > 0:\n",
    "                perm = torch.randperm(len(pool))[:extras]\n",
    "                idxs.append(pool[perm])\n",
    "        idx = torch.cat(idxs) if len(idxs) > 0 else torch.tensor([], device=valid.device, dtype=valid.dtype)\n",
    "        if len(idx) > 0:\n",
    "            x[b, idx, 2:7] = 0.0\n",
    "            masked[b, idx] = True\n",
    "    return masked\n",
    "\n",
    "\n",
    "class MPTModel(nn.Module):\n",
    "    def __init__(self, base_enc: BaselineCLS):\n",
    "        super().__init__()\n",
    "        self.encoder = base_enc.encoder\n",
    "        d = base_enc.in_proj.out_features\n",
    "        self.head_flux = nn.Linear(d, 1)\n",
    "        self.head_band = nn.Linear(d, 3)\n",
    "        self.head_dt = nn.Linear(d, 1)\n",
    "        self.cls_tok = base_enc.cls_tok\n",
    "        self.in_proj = base_enc.in_proj\n",
    "        self.time2vec = base_enc.time2vec\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.head_flux(z), self.head_band(z), self.head_dt(z)\n",
    "\n",
    "\n",
    "def run_pretraining(cfg: SimpleNamespace, outfile=\"mpt_encoder.pt\"):\n",
    "    out_dir = Path(cfg.output_dir)\n",
    "    train_df = pd.read_csv(out_dir / \"manifest_train.csv\")\n",
    "    ds_pt = PhotoEventDataset(train_df, horizon=100)\n",
    "    mean, std = load_stats(out_dir / cfg.stats_file)\n",
    "    collate = build_collate(mean, std)\n",
    "\n",
    "    g = None\n",
    "    if cfg.strict_seed:\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(cfg.seed)\n",
    "\n",
    "    ld_pt = DataLoader(\n",
    "        ds_pt,\n",
    "        batch_size=PT_BATCH,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=True,\n",
    "        generator=g,\n",
    "        worker_init_fn=seed_worker if cfg.strict_seed else None,\n",
    "    )\n",
    "\n",
    "    enc = BaselineCLS(cfg.d_model, cfg.n_heads, cfg.n_layers, NUM_CLASSES, cfg.dropout, cfg.max_len).to(\n",
    "        device\n",
    "    )\n",
    "    mpt = MPTModel(enc).to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(\n",
    "        [\n",
    "            {\"params\": enc.encoder.parameters(), \"lr\": PT_LR * 0.5},\n",
    "            {\n",
    "                \"params\": list(mpt.head_flux.parameters())\n",
    "                + list(mpt.head_band.parameters())\n",
    "                + list(mpt.head_dt.parameters()),\n",
    "                \"lr\": PT_LR,\n",
    "            },\n",
    "        ],\n",
    "        weight_decay=1e-2,\n",
    "    )\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=PT_EPOCHS)\n",
    "\n",
    "    for ep in range(1, PT_EPOCHS + 1):\n",
    "        mpt.train()\n",
    "        ep_tot = 0.0\n",
    "        for x_mask, _, pad_mask in ld_pt:\n",
    "            x_mask, pad_mask = x_mask.to(device), pad_mask.to(device)\n",
    "            x_orig = x_mask.clone()\n",
    "            masked_tok = _mask_batch(x_mask, pad_mask)\n",
    "\n",
    "            emb = enc.in_proj(x_mask)\n",
    "            te = F.dropout(enc.time2vec(x_mask[..., 0]), p=cfg.p_dropout) if enc.time2vec is not None else 0.0\n",
    "            h_in = emb + te if enc.time2vec is not None else emb\n",
    "            B = h_in.size(0)\n",
    "            tok = enc.cls_tok.expand(B, 1, -1)\n",
    "            h = torch.cat([tok, h_in], 1)\n",
    "            pad = torch.cat([pad_mask.new_zeros((B, 1)), pad_mask], 1)\n",
    "\n",
    "            z_full = enc.encoder(h, src_key_padding_mask=pad)\n",
    "            h_masked = z_full[:, 1:]\n",
    "            f_hat, b_hat, dt_hat = mpt(h_masked)\n",
    "            mf = masked_tok.view(-1)\n",
    "\n",
    "            true_f = x_orig[..., 2].view(-1)\n",
    "            loss_f = F.mse_loss(f_hat.view(-1)[mf], true_f[mf])\n",
    "            true_b = x_orig[..., 4:7].argmax(-1).view(-1)\n",
    "            loss_b = F.cross_entropy(b_hat.view(-1, 3)[mf], true_b[mf])\n",
    "            dt_gt = torch.roll(x_orig[..., 1], -1, dims=1)\n",
    "            dt_gt[:, -1] = 0.0\n",
    "            dt_gt = dt_gt.view(-1)\n",
    "            loss_dt = F.mse_loss(dt_hat[..., 0].view(-1)[mf], dt_gt[mf])\n",
    "\n",
    "            loss = LAMBDA_F * loss_f + LAMBDA_B * loss_b + LAMBDA_DT * loss_dt\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(mpt.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            ep_tot += float(loss.item())\n",
    "\n",
    "        sched.step()\n",
    "        print(f\"[MPT] EP{ep:02d} | loss {ep_tot/len(ld_pt):.4f}\")\n",
    "\n",
    "    torch.save(enc.state_dict(), outfile)\n",
    "    print(\"Saved pretrained encoder to\", outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c442c4d-6b29-44b7-af0e-3ace8f541366",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c741bcb-bb3c-43d9-8d70-b4c163673ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Example Smoke-test\n",
    "# =========================\n",
    "# This runs:\n",
    "#  1) Single finetune\n",
    "#  2) Calibration\n",
    "#  3) Tiny HParam search (if enabled)\n",
    "#  4) Calibration of best trial\n",
    "#  5) K-Fold cross-validation (if enabled)\n",
    "\n",
    "# (A) Single finetune\n",
    "print(\">> SINGLE FINETUNE\")\n",
    "out_single = run_finetune(CFG)\n",
    "plot_loss_curves(out_single[\"curves\"], title=f\"{CFG.finetune_strategy.upper()} fine-tuning\")\n",
    "# Evaluate & plot test\n",
    "_, _, test_ld = out_single[\"loaders\"]\n",
    "m_single, (y_t, P_t, L_t) = summarize_on_loader(out_single[\"model\"], test_ld)\n",
    "print(\"[TEST single] \", m_single)\n",
    "plot_confusion(y_t, P_t.argmax(1), title=\"Test Confusion (single)\")\n",
    "plot_roc_pr(y_t, P_t, title_prefix=\"Test (single)\")\n",
    "\n",
    "# (B) Calibration after single\n",
    "cal_single = calibrate_if_enabled(CFG, out_single[\"model\"], out_single[\"loaders\"])\n",
    "if cal_single is not None:\n",
    "    print(f\"Temperature (single): T={cal_single['T']:.3f}\")\n",
    "    # Plot reliability (VAL pre/post)\n",
    "    plot_reliability_diagram(cal_single[\"val\"][\"bins_unc\"], title=\"Reliability (VAL, uncal)\")\n",
    "    plot_reliability_diagram(\n",
    "        cal_single[\"val\"][\"bins_cal\"], title=f\"Reliability (VAL, temp) T={cal_single['T']:.2f}\"\n",
    "    )\n",
    "    # one could re-plot ROC/PR...\n",
    "\n",
    "# (C) Tiny HParam search (very fast for smoke test)\n",
    "print(\">> HYPERPARAMETER SEARCH\")\n",
    "best = None\n",
    "if CFG.hps_enabled:\n",
    "    (best_cfg, best_out), trials = tiny_hparam_search(\n",
    "        copy.deepcopy(CFG), grid=CFG.hps_grid, max_trials=CFG.hps_max_trials\n",
    "    )\n",
    "    best = (best_cfg, best_out)\n",
    "    plot_loss_curves(best_out[\"curves\"], title=\"HParam best fine-tuning\")\n",
    "    _, _, test_ld_best = best_out[\"loaders\"]\n",
    "    m_best, (y_best, P_best, L_best) = summarize_on_loader(best_out[\"model\"], test_ld_best)\n",
    "    print(\"[TEST best HParam] \", m_best)\n",
    "    plot_confusion(y_best, P_best.argmax(1), title=\"Test Confusion (best HParam)\")\n",
    "    plot_roc_pr(y_best, P_best, title_prefix=\"Test (best HParam)\")\n",
    "\n",
    "    # (D) Calibration of best\n",
    "    cal_best = calibrate_if_enabled(best_cfg, best_out[\"model\"], best_out[\"loaders\"])\n",
    "    if cal_best is not None:\n",
    "        print(f\"Temperature (best): T={cal_best['T']:.3f}\")\n",
    "        plot_reliability_diagram(cal_best[\"val\"][\"bins_unc\"], title=\"Reliability (VAL, uncal, best)\")\n",
    "        plot_reliability_diagram(\n",
    "            cal_best[\"val\"][\"bins_cal\"], title=f\"Reliability (VAL, temp, best) T={cal_best['T']:.2f}\"\n",
    "        )\n",
    "\n",
    "# (E) K-fold CV\n",
    "if CFG.k_folds and CFG.k_folds >= 2:\n",
    "    print(\"\\n>> K-FOLD CROSS-VALIDATION\")\n",
    "    df_cv, preds_cv = run_crossval_finetune(copy.deepcopy(CFG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4034ae-34c4-49c0-9819-c4edc1c03942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553b5b6-98c2-4217-a4df-df1dda3f035f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc99b40-fe3f-42f9-8705-e9d0d38a417f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-applecider_gpu]",
   "language": "python",
   "name": "conda-env-.conda-applecider_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
