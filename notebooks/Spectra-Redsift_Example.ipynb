{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick Demo toggle: set to True for a fast, CPU-friendly run\n",
        "quick_demo = False  # change to True to run a tiny demo\n",
        "demo_note = \"Quick Demo is ON\" if quick_demo else \"Quick Demo is OFF\"\n",
        "print(demo_note)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SpectraNet Redshift — Example Walkthrough\n",
        "\n",
        "This notebook is a beginner-friendly example to understand and run SpectraNet for spectroscopic redshift regression.\n",
        "\n",
        "What you'll do:\n",
        "- Set up environment and imports\n",
        "\n",
        "- Configure dataset paths and training hyperparameters\n",
        "\n",
        "- Peek at a few samples to understand the input format\n",
        "\n",
        "- Build and summarize the model\n",
        "\n",
        "- Train/evaluate (optionally with a quick demo mode)\n",
        "\n",
        "- Run a small inference demo and visualize predictions vs ground truth\n",
        "\n",
        "> Tip: If you're new or running on CPU, enable the Quick Demo mode in the next cell to make it fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b0-C77y0N1p"
      },
      "outputs": [],
      "source": [
        "# Setup and imports (GPU-ready and repo-aware)\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "max_length = 1_000_000\n",
        "InteractiveShell.instance().display_formatter.formatters['text/plain'].max_seq_length = int(max_length)\n",
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "\n",
        "from collections import deque\n",
        "import os\n",
        "import sys\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Import SpectraNet redshift model from this repo structure\n",
        "try:\n",
        "    from AppleCider.models.SpectraNetRedshift import build_spec_red_model\n",
        "    from AppleCider.preprocess.data_loader_redshift import create_data_loaders\n",
        "    from AppleCider.preprocess.utils_redshift import (\n",
        "        set_seed as train_set_seed,\n",
        "        get_device as train_get_device,\n",
        "        train_one_epoch_regression,\n",
        "        validate_regression,\n",
        "        build_optimizer,\n",
        "        build_scheduler,\n",
        "        early_stopping,\n",
        "        clean_memory,\n",
        "    )\n",
        "except ModuleNotFoundError:\n",
        "    # Fallback if run from repository root\n",
        "    sys.path.append(os.path.abspath('.'))\n",
        "    from AppleCider.models.SpectraNetRedshift import build_spec_red_model\n",
        "    from AppleCider.preprocess.data_loader_redshift import create_data_loaders\n",
        "    from AppleCider.preprocess.utils_redshift import (\n",
        "        set_seed as train_set_seed,\n",
        "        get_device as train_get_device,\n",
        "        train_one_epoch_regression,\n",
        "        validate_regression,\n",
        "        build_optimizer,\n",
        "        build_scheduler,\n",
        "        early_stopping,\n",
        "        clean_memory,\n",
        "    )\n",
        "\n",
        "# Utility functions for this example (kept lightweight)\n",
        "\n",
        "def get_device():\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def make_dummy_loader(batch_size=8, num_batches=2, length=4096, seed=0):\n",
        "    \"\"\"Create a tiny synthetic dataset shaped like spectra: [B, 1, 4096].\"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    for _ in range(num_batches):\n",
        "        x = rng.normal(0, 1, size=(batch_size, 1, length)).astype(np.float32)\n",
        "        # Create a smooth target roughly correlated with low-frequency content\n",
        "        y = (x.mean(axis=-1, keepdims=False) + 0.1 * rng.randn(batch_size, 1)).clip(0, None).astype(np.float32)\n",
        "        yield torch.from_numpy(x), torch.from_numpy(y.squeeze(1))\n",
        "\n",
        "\n",
        "def quick_metrics(y_true: torch.Tensor, y_pred: torch.Tensor):\n",
        "    y_true = y_true.detach().cpu().float()\n",
        "    y_pred = y_pred.detach().cpu().float()\n",
        "    mse = torch.mean((y_pred - y_true) ** 2).item()\n",
        "    mae = torch.mean(torch.abs(y_pred - y_true)).item()\n",
        "    bias = torch.mean(y_pred - y_true).item()\n",
        "    return {\"mse\": mse, \"mae\": mae, \"bias\": bias}\n",
        "\n",
        "\n",
        "def plot_one_spectrum(x: torch.Tensor, title: str = \"Example spectrum\"):\n",
        "    x = x.detach().cpu().numpy().reshape(-1)\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.plot(x, lw=1.0)\n",
        "    plt.xlabel('Wavelength index')\n",
        "    plt.ylabel('Flux (arb. units)')\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example: minimal forward pass demo for sanity check\n",
        "\n",
        "def forward_pass_demo(seed=42):\n",
        "    set_seed(seed)\n",
        "    device = get_device()\n",
        "    model = build_spec_red_model(config={}).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Use a tiny synthetic batch\n",
        "    loader = make_dummy_loader(batch_size=4, num_batches=1, length=4096, seed=seed)\n",
        "    x, y = next(iter(loader))\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(x)\n",
        "    metrics = quick_metrics(y, y_pred)\n",
        "\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Model parameters: {count_parameters(model):,}\")\n",
        "    print(\"Synthetic batch: x ->\", tuple(x.shape), \", y ->\", tuple(y.shape))\n",
        "    print(\"Quick metrics on synthetic data:\", {k: round(v, 6) for k, v in metrics.items()})\n",
        "\n",
        "    # Visualize one spectrum\n",
        "    plot_one_spectrum(x[0], title=\"Synthetic spectrum example\")\n",
        "\n",
        "    return model, (x, y, y_pred), metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCDwv-6N0N1t"
      },
      "outputs": [],
      "source": [
        "# 3) Configuration (paths and hyperparameters)\n",
        "\n",
        "# Base directory for your redshift dataset (adjust to your environment)\n",
        "# Expected files prepared by preprocessing:\n",
        "#   <redshift_dir>/processed/train.pt\n",
        "#   <redshift_dir>/processed/val.pt\n",
        "#   <redshift_dir>/processed/test.pt\n",
        "\n",
        "data_dir_base = os.path.join(project_root, 'data')  # change if your data lives elsewhere\n",
        "redshift_dir = os.path.join(data_dir_base, 'dataset_for_redshift_with_error')\n",
        "processed_dir = os.path.join(redshift_dir, 'processed')\n",
        "model_save_dir = os.path.join(redshift_dir, 'model')\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "final_config = {\n",
        "    \"batch_size\": 256,\n",
        "    \"learning_rate\": 1.6e-4,\n",
        "    \"weight_decay\": 1e-5,\n",
        "\n",
        "    # scheduler\n",
        "    \"warmup_epochs\": 10,\n",
        "    \"T_0\": 5,\n",
        "    \"T_mult\": 1,\n",
        "    \"eta_min\": 1e-5,\n",
        "    \"start_factor\": 1e-6,\n",
        "    \"end_factor\": 1.0,\n",
        "\n",
        "    \"num_workers\": 4,\n",
        "    \"epochs\": 100,\n",
        "    \"patience\": 5,\n",
        "\n",
        "    # Processed PT files for loaders\n",
        "    \"train_dir\": os.path.join(processed_dir, 'train.pt'),\n",
        "    \"val_dir\": os.path.join(processed_dir, 'val.pt'),\n",
        "    \"test_dir\": os.path.join(processed_dir, 'test.pt'),\n",
        "\n",
        "    \"model_save_dir\": model_save_dir,\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "print(\"Config summary:\")\n",
        "for k, v in final_config.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup\n",
        "\n",
        "Below we import dependencies, wire up this repo for imports, and define a few tiny helpers for reproducibility, quick metrics, and plotting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Peek at the data\n",
        "\n",
        "We'll first run a tiny synthetic example with spectra shaped like `[B, 1, 4096]`. If you have processed tensors, you can wire your real loaders here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a small synthetic batch and visualize\n",
        "\n",
        "set_seed(final_config.get(\"seed\", 42))\n",
        "loader = make_dummy_loader(batch_size=4 if quick_demo else 8, num_batches=1, length=4096, seed=final_config.get(\"seed\", 42))\n",
        "x_demo, y_demo = next(iter(loader))\n",
        "print(\"Synthetic demo shapes:\", x_demo.shape, y_demo.shape)\n",
        "plot_one_spectrum(x_demo[0], title=\"Synthetic spectrum example (peek)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Build the model\n",
        "\n",
        "We use `build_spec_red_model(config)` to construct the SpectraNet regression model. It takes spectra shaped `[B, 1, 4096]` and outputs a non-negative redshift via a softplus head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build model and inspect parameters\n",
        "device = get_device()\n",
        "model = build_spec_red_model(config={}).to(device)\n",
        "print(model.__class__.__name__)\n",
        "print(f\"Trainable parameters: {count_parameters(model):,}\")\n",
        "\n",
        "_ = model.eval()\n",
        "# quick forward with synthetic batch\n",
        "x_demo = x_demo.to(device)\n",
        "with torch.no_grad():\n",
        "    y_hat_demo = model(x_demo)\n",
        "print(\"Forward OK. Output shape:\", tuple(y_hat_demo.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Inference demo\n",
        "\n",
        "Let’s run a tiny forward pass on synthetic data to see predictions vs. targets and basic metrics. If you have a trained checkpoint, you can load it before inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the minimal forward pass demo (synthetic)\n",
        "\n",
        "_ = forward_pass_demo(seed=final_config.get(\"seed\", 42))\n",
        "\n",
        "# Optional: scatter of y vs y_pred for the small synthetic batch\n",
        "device = get_device()\n",
        "model = build_spec_red_model(config={}).to(device)\n",
        "model.eval()\n",
        "loader = make_dummy_loader(batch_size=16 if quick_demo else 64, num_batches=1, length=4096, seed=final_config.get(\"seed\", 42))\n",
        "x, y = next(iter(loader))\n",
        "x, y = x.to(device), y.to(device)\n",
        "with torch.no_grad():\n",
        "    y_pred = model(x)\n",
        "m = quick_metrics(y, y_pred)\n",
        "print({k: round(v, 6) for k, v in m.items()})\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.scatter(y.cpu().numpy(), y_pred.cpu().numpy(), s=12, alpha=0.6)\n",
        "plt.xlabel(\"Target redshift (synthetic)\")\n",
        "plt.ylabel(\"Predicted redshift\")\n",
        "plt.title(\"Tiny synthetic scatter\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use your own data\n",
        "\n",
        "To run with real spectra instead of the synthetic demo:\n",
        "- Place your processed tensors in: `data/dataset_for_redshift_with_error/processed/{train,val,test}.pt`\n",
        "- Each tensor should contain spectra shaped `[B, 1, 4096]` and targets shaped `[B]` (float redshift).\n",
        "- Update `data_dir_base` or `redshift_dir` in the config cell if your data lives elsewhere.\n",
        "- Replace the synthetic loader with your own DataLoader wiring before training/evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How SpectraNet works (conceptual)\n",
        "\n",
        "- Input: 1D spectra of fixed length 4096 as `[B, 1, 4096]`.\n",
        "- Backbone: multi-branch 1D conv blocks per stage with several kernel sizes to capture narrow/wide features (lines and continuum).\n",
        "- Normalization: LayerNorm over channel dimension to stabilize across wavelengths.\n",
        "- Downsampling: periodic max-pooling to compress sequence length.\n",
        "- Head: global pooling + MLP regressor, with a softplus at the end to ensure non-negative redshift.\n",
        "\n",
        "This example uses a tiny synthetic dataset. With real data, ensure your wavelength grid and flux normalization match the expected format.\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "- ImportError for `AppleCider`: ensure this notebook runs from the `notebooks/` folder so `..` points to repo root.\n",
        "- CUDA out of memory: set `quick_demo = True` to use small batches and CPU.\n",
        "- Different spectrum length: resample or pad/crop to 4096 and keep the channel dimension as 1.\n",
        "- Bad metrics on synthetic data: this is expected; use real data and training for meaningful results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Train and validate (full run)\n",
        "\n",
        "Now we’ll use the real processed datasets and train SpectraNet end-to-end on GPU (if available). We log per-epoch metrics and keep the best model by validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build loaders, model, optimizer, scheduler, and run training\n",
        "train_set_seed(final_config[\"seed\"])\n",
        "device = train_get_device()\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Data\n",
        "train_loader, val_loader, test_loader = create_data_loaders({\n",
        "    \"train_dir\": final_config[\"train_dir\"],\n",
        "    \"val_dir\": final_config[\"val_dir\"],\n",
        "    \"test_dir\": final_config[\"test_dir\"],\n",
        "    \"batch_size\": final_config[\"batch_size\"],\n",
        "    \"num_workers\": final_config[\"num_workers\"],\n",
        "})\n",
        "\n",
        "# Model and optimization\n",
        "model = build_spec_red_model(config={}).to(device)\n",
        "optimizer = build_optimizer(model, final_config)\n",
        "scheduler = build_scheduler(optimizer, final_config)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "best_val = float('inf')\n",
        "epochs_no_improve = 0\n",
        "best_path = os.path.join(final_config[\"model_save_dir\"], \"best_model.pth\")\n",
        "\n",
        "for epoch in range(1, final_config[\"epochs\"] + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{final_config['epochs']}\")\n",
        "    train_loss = train_one_epoch_regression(model, train_loader, optimizer, device, scaler, loss_fn)\n",
        "    val_stats = validate_regression(model, val_loader, device, loss_fn, plot=False)\n",
        "    val_loss = val_stats['loss']\n",
        "    print({\"train_loss\": round(train_loss, 6), **{k: round(v, 6) for k, v in val_stats.items()}})\n",
        "    \n",
        "    scheduler.step()\n",
        "    \n",
        "    # Early stopping check\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(f\"Saved best model to {best_path}\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if early_stopping(epochs_no_improve, final_config['patience']):\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "    \n",
        "    clean_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Evaluate on test set\n",
        "\n",
        "Load the best checkpoint and compute full regression metrics. Also plot predictions vs. ground truth with the ±0.15(1+z) bands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model and evaluate on test set with plot\n",
        "best_model = build_spec_red_model(config={}).to(device)\n",
        "best_state = torch.load(os.path.join(final_config[\"model_save_dir\"], \"best_model.pth\"), map_location=device)\n",
        "best_model.load_state_dict(best_state)\n",
        "best_model.eval()\n",
        "\n",
        "test_stats = validate_regression(best_model, test_loader, device, torch.nn.MSELoss(), plot=True)\n",
        "print({k: round(v, 6) for k, v in test_stats.items()})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AppleCider",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
