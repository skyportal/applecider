import numpy as np
import pandas as pd
import os
import shutil

from tqdm.auto import tqdm
import matplotlib.pyplot as plt

from pathlib import Path
from types import SimpleNamespace

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset, Subset
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
from torchvision.ops import SqueezeExcitation

from astropy.coordinates import SkyCoord

from sklearn.metrics import precision_recall_curve, auc, confusion_matrix
from sklearn.model_selection import StratifiedShuffleSplit

import unittest
import tempfile
from collections import defaultdict

import math
import random
import math

import inspect

import timm
from timm.models import convnext

from matplotlib.gridspec import GridSpec

from astropy.coordinates import SkyCoord
import astropy.units as u

from einops import rearrange
from torch.nn import MultiheadAttention

from sklearn.metrics import precision_recall_curve, auc, roc_auc_score, roc_curve

from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
from sklearn.metrics import auc as sklearn_auc
import random
from datetime import datetime as t
import argparse

import copy
import json


CLASSES = [['SN Ia','SN Ic','SN Ib'],[ 'SN IIP', 'SN IIn','SN II', 'SN IIb'], ['Cataclysmic'], ['AGN'], ['Tidal Disruption Event']]

# SHOW_CLASSES =['AGN', 'Tidal Disruption Event', 'SN Ia', 'SN Ic', 'SN IIP', 'SN IIn','SN II','SN Ib', 'Cataclysmic']
# SHOW_CLASSES =['AGN', 'Tidal Disruption Event', 'SN Ia', 'other SN', 'Cataclysmic']
# SHOW_CLASSES = ['Nuclear', 'SN', 'Cataclysmic']
SHOW_CLASSES =['SN I', 'SN II', 'Cataclysmic', 'AGN','Tidal Disruption Event']


device = torch.device("cuda")

# GPU Selection Logic
def select_gpu(gpu=None,min_free_memory_gb=22):
    if not gpu==None:
        print(f'cuda:{gpu}')
        return f'cuda:{gpu}'
    if not torch.cuda.is_available():
        return 'cpu'
    
    for i in range(torch.cuda.device_count()):
        free = torch.cuda.mem_get_info(i)[0] / (1024 ** 3)
        if free >= min_free_memory_gb:
            torch.cuda.set_device(i)
            print(f'cuda:{i}')
            return f'cuda:{i}'
    
    print("No suitable GPU found, using CPU")
    return 'cpu'


def calculate_val_loss(loader, model, criterion, DEVICE):
    """Calculate validation loss using the same criterion as training.
    
    Args:
        loader: DataLoader for validation data
        model: Model to evaluate
        criterion: Loss function (same as used in training)
        DEVICE: Device to run calculations on
    
    Returns:
        float: Average validation loss
    """
    model.eval()
    val_loss = 0.0
    
    with torch.no_grad():
        for batch in loader:
            metadata = batch['metadata'].to(DEVICE)
            image = batch['image'].to(DEVICE)
            target = batch['target'].to(DEVICE)
            
            outputs = model(metadata, image)
            loss = criterion(outputs['logits'], target)
            val_loss += loss.item()
    
    return val_loss / len(loader)

def get_class_counts(dataloader):
    num_classes = len(CLASSES)  # Always use coarse classes
    counts = torch.zeros(num_classes, dtype=torch.long)
    
    for batch in dataloader:
        targets = batch['target']
        # For one-hot encoded targets (assuming your targets are one-hot)
        if targets.dim() == 2:
            counts += targets.sum(dim=0).long()
        # For class index targets (alternative format)
        else:
            class_indices = targets.argmax(dim=1) if targets.dim() == 2 else targets
            counts += torch.bincount(class_indices, minlength=num_classes)
    # print(counts)
    
    return counts



class CNN_tower(nn.Module):
    def __init__(self, output_dims=512, img_size=49):
        super().__init__()
        self.output_dims = output_dims
        self.img_size = img_size
        
        # --- Three Independent Backbones ---
        def make_backbone():
            return nn.Sequential(
                nn.Conv2d(1, 32, kernel_size=3, padding='same'),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(32, 64, kernel_size=3, padding='same'),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(64, 128, kernel_size=3,  padding='same'),
                nn.ReLU()
            )
        
        self.backbone_ch0 = make_backbone()  # Galaxy 1
        
        # Positional embeddings (only used for ch0 and ch2)
        self.pos_embedding = PositionEmbeddingSine(128, img_size//4)
        self.coord_conv = nn.Conv2d(128, 128, kernel_size=1)
        
        # Attention (only for ch0 and ch2)
        self.attention = nn.Sequential(
            nn.Conv2d(128, 128, kernel_size=1),
            nn.ReLU(),
            nn.Conv2d(128, 1, kernel_size=1),
            nn.Sigmoid()
        )
        
        # Projection for channel 1 (no position/attention)
        self.proj_ch1 = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(128, 128)
        )
        
        # Combined projection (ch0+ch2 features + offsets + ch1 features)
        self.proj = nn.Sequential(
            nn.Linear(128*3, 256),  # +2 for dx/dy between ch0 and ch2
            nn.GELU(),
            nn.LayerNorm(256),
            nn.Linear(256, output_dims)
        )
        
        self.fusion_router_2 = nn.Sequential(
            # Conv Block 1
            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # [16, 63, 63]
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),  # [16, 31, 31]
            
            # Conv Block 2
            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # [32, 31, 31]
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),  # [32, 15, 15]
            
            # Conv Block 3
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # [64, 15, 15]
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),  # [64, 7, 7]
            
            # Final layers
            nn.Flatten(),
            nn.Linear(64 * 7 * 7, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 2),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        B, C, H, W = x.shape
        device = x.device
        
        # Process each channel with its own backbone
        feats_ch0 = self.backbone_ch0(x[:, 0:1])  # (B, 128, H', W')
        feats_ch1 = self.backbone_ch0(x[:, 1:2])  # (B, 128, H', W')
        feats_ch2 = self.backbone_ch0(x[:, 2:3])  # (B, 128, H', W')
        
        # --- Positional logic for ch0 and ch2 only ---
        pos_encoding = self.pos_embedding(B, H//4, W//4, device)
        pos_features = self.coord_conv(pos_encoding)
        
        # Attended features for ch0 and ch2
        feats_ch0 = feats_ch0 + pos_features
        feats_ch2 = feats_ch2 + pos_features
        
        attn_ch0 = self.attention(feats_ch0)
        attn_ch0 = attn_ch0 / (attn_ch0.sum(dim=(2,3), keepdim=True) + 1e-8)
        attn_ch2 = self.attention(feats_ch2)
        attn_ch2 = attn_ch2 / (attn_ch2.sum(dim=(2,3), keepdim=True) + 1e-8)
        
        
        # Centroids for ch0 and ch2
        def get_centroid(attn):
            B, _, H_, W_ = attn.shape
            grid_x = torch.linspace(-1, 1, W_).to(device).view(1, 1, 1, W_).expand(B, -1, H_, -1)
            grid_y = torch.linspace(-1, 1, H_).to(device).view(1, 1, H_, 1).expand(B, -1, -1, W_)
            cx = (attn * grid_x).sum(dim=(2,3))  # (B, 1)
            cy = (attn * grid_y).sum(dim=(2,3))  # (B, 1)
            return torch.cat([cx, cy], dim=1)  # (B, 2)
        
        centroid_ch0 = get_centroid(attn_ch0)
        centroid_ch2 = get_centroid(attn_ch2)
        
        # Relative offset (ch0 to ch2)
        dx = centroid_ch0[:, 0] - centroid_ch2[:, 0]
        dy = centroid_ch0[:, 1] - centroid_ch2[:, 1]
        offsets = torch.stack([dx, dy], dim=1)

        # With:
        distance = torch.sqrt((centroid_ch0[:,0] - centroid_ch2[:,0])**2 + 
                            (centroid_ch0[:,1] - centroid_ch2[:,1])**2)
        angle = torch.atan2(dy, dx)  # Optional (if angle matters)
        offsets = distance.unsqueeze(1)  # (B, 1) instead of (B, 2)
        
        # Channel 1: Simple pooled features (no position/attention)
        pooled_ch1 = self.proj_ch1(feats_ch1)  # (B, 128)
        
        # Combine features: ch0/ch2 centroids + offsets + ch1 features
        feats_ch0 = feats_ch0.sum(dim=(2,3))  # (B, 128)
        feats_ch2 = feats_ch2.sum(dim=(2,3))  # (B, 128)
        combined = torch.cat([feats_ch0, feats_ch2, pooled_ch1], dim=1)  # (B, 128*3 + 2)
        
        output = self.proj(combined)
        return output




class PositionEmbeddingSine(nn.Module):
    """
    This is a bit of a frankenversion of the position embedding from "Attention Is All You Need"
    and "End-to-End Object Detection with Transformers" (DETR).
    It combines sine/cosine positional encodings with learned features.
    """
    def __init__(self, num_pos_feats=128, temperature=10000, normalize=False, scale=None):
        super().__init__()
        self.num_pos_feats = num_pos_feats
        self.temperature = temperature
        self.normalize = normalize
        if scale is not None and normalize is False:
            raise ValueError("normalize should be True if scale is passed")
        if scale is None:
            scale = 2 * math.pi
        self.scale = scale

        # Learned components
        self.learned_embedding = nn.Sequential(
            nn.Conv2d(2, num_pos_feats//2, kernel_size=1),
            nn.GELU(),
            nn.Conv2d(num_pos_feats//2, num_pos_feats, kernel_size=1)
        )
        
        # Fourier feature mapping
        self.fourier_proj = nn.Linear(4, num_pos_feats//2)
        # self.fourier_proj = nn.Linear(4, num_pos_feats//2)
        self._reset_parameters()

    def _reset_parameters(self):
        nn.init.uniform_(self.fourier_proj.weight, 0.0, 1.0)
        nn.init.zeros_(self.fourier_proj.bias)

    def forward(self, batch_size, height, width, device):
        if self.normalize:
            y_embed = torch.linspace(-1, 1, height, device=device)
            x_embed = torch.linspace(-1, 1, width, device=device)
        else:
            y_embed = torch.arange(height, device=device).float()
            x_embed = torch.arange(width, device=device).float()

        if self.normalize:
            y_embed = y_embed * self.scale
            x_embed = x_embed * self.scale

        # Basic coordinate grid
        dim_t = torch.arange(self.num_pos_feats // 2, dtype=torch.float32, device=device)
        dim_t = self.temperature ** (2 * (dim_t // 2) / (self.num_pos_feats // 2))

        pos_x = x_embed[:, None] / dim_t  # (width, num_pos_feats//2)
        pos_y = y_embed[:, None] / dim_t  # (height, num_pos_feats//2)
        
        # Sine/cosine positional encodings
        pos_x = torch.stack((pos_x[:, 0::2].sin(), pos_x[:, 1::2].cos()), dim=2).flatten(1)  # (width, num_pos_feats//2)
        pos_y = torch.stack((pos_y[:, 0::2].sin(), pos_y[:, 1::2].cos()), dim=2).flatten(1)  # (height, num_pos_feats//2)
        
        # Create 2D positional encodings
        pos_x = pos_x.unsqueeze(0).expand(height, -1, -1)  # (height, width, num_pos_feats//2)
        pos_y = pos_y.unsqueeze(1).expand(-1, width, -1)    # (height, width, num_pos_feats//2)
        pos = torch.cat([pos_y, pos_x], dim=-1)            # (height, width, num_pos_feats)
        
        # Permute to channel-first format and add batch dimension
        pos = pos.permute(2, 0, 1).unsqueeze(0).expand(batch_size, -1, -1, -1)  # (B, C, H, W)
        
        # Create basic coordinate maps for learned embedding
        if self.normalize:
            y_coords = torch.linspace(-1, 1, height, device=device)
            x_coords = torch.linspace(-1, 1, width, device=device)
        else:
            y_coords = torch.arange(height, device=device).float() / height
            x_coords = torch.arange(width, device=device).float() / width
            
        yy, xx = torch.meshgrid(y_coords, x_coords, indexing='ij')
        coord_map = torch.stack([xx, yy], dim=0).unsqueeze(0).expand(batch_size, -1, -1, -1)  # (B, 2, H, W)
        
        # Learned embedding component
        learned_pos = self.learned_embedding(coord_map)  # (B, C, H, W)
        
        # Fourier feature mapping
        fourier_input = torch.cat([
            torch.sin(coord_map * 2 * math.pi),
            torch.cos(coord_map * 2 * math.pi)
        ], dim=1)  # (B, 4, H, W)
        fourier_input = fourier_input.permute(0, 2, 3, 1)  # (B, H, W, 4)
        fourier_features = self.fourier_proj(fourier_input)  # (B, H, W, C/2)
        fourier_features = fourier_features.permute(0, 3, 1, 2)  # (B, C/2, H, W)
        
        # Combine all positional information
        full_pos = torch.cat([
            pos,
            learned_pos,
            fourier_features
        ], dim=1)  # (B, 2.5*C, H, W)
        
        # Project down to original dimension
        return full_pos[:, :self.num_pos_feats]  # (B, C, H, W)
    
    
class SplitHeadConvNeXt(nn.Module):
    # original: def __init__(self, pretrained=False, in_chans=4, outdims=4):
    def __init__(self, pretrained=False, in_chans=4, outdims=4):
        super().__init__()
        # Load base model (disable default head)
        self.backbone = timm.create_model(
            'convnext_tiny',
            pretrained=pretrained,
            in_chans=in_chans,
            num_classes=0  # Disable original classifier
        )
        
        # Get feature dimension (e.g., 768 for 'convnext_tiny')
        features = self.backbone.num_features
        
        # Define two separate heads
        self.head_main = nn.Sequential(
            nn.GELU(),
            nn.LayerNorm(features),
            nn.Linear(features, features//2),    # 4-dimensional output
            nn.ReLU(),
            nn.Dropout(.4),
            nn.Linear(features//2, features),
            nn.Linear( features, outdims)
        )
        
        self.head_aux = nn.Sequential(
            nn.LayerNorm(features),
            nn.Linear(features, outdims),
            nn.Tanh()
            
            )     # 1-dimensional output

    def forward(self, x):
        features = self.backbone(x)  # (batch_size, features)
        out = self.head_main(features)  # (batch_size, 4)
        activation = self.head_aux(features)    # (batch_size, 1)
        return out*activation

    
    
    
COLORS = ['#1f77b4', '#ff7f0e', '#2ca02c', '#c702b6', '#2ca02c','#1f77b4', '#ff7f0e', '#2ca02c', '#c702b6', '#2ca02c']

def get_model_predictions(loader, model, device):
    """Get model predictions and targets for coarse classification"""
    model.eval()
    all_probs = []
    all_targets = []
    
    with torch.no_grad():
        # model.fusion_router.set_testing_mode(True)
        for batch in loader:
            metadata = batch['metadata'].to(device)
            image = batch['image'].to(device)
            
            outputs = model(metadata, image)
            
            # Get probabilities from logits using 
            probs = torch.softmax(outputs['logits'], dim=-1).cpu().numpy()
            targets = batch['target'].cpu().numpy()
            
            all_probs.append(probs)
            all_targets.append(targets)
    
    return np.concatenate(all_probs), np.concatenate(all_targets)


def plot_pr_curves(ax, probs, targets):
    """Plot precision-recall curves with correct color mapping"""
    colors = plt.cm.tab10.colors  # Use a built-in qualitative colormap
    
    for class_idx, class_name in enumerate(CLASSES):
        precision, recall, _ = precision_recall_curve(
            targets[:, class_idx],
            probs[:, class_idx]
        )
        ax.plot(recall, precision, 
               color=colors[class_idx % len(colors)],  # Auto-scale to class count
               lw=2, 
               label=f'{class_name} (AUC={auc(recall, precision):.2f}')  # Show AUC in legend
    
    ax.set_xlabel('Recall')
    ax.set_ylabel('Precision')
    ax.set_title('Precision-Recall Curves')
    ax.legend(loc='center left')
    ax.grid(True, alpha=0.3)
    ax.set_xlim([0.0, 1.05])
    ax.set_ylim([0.0, 1.05])
    
    
def plot_confusion_matrix(ax, targets, y_pred):
    """Plot normalized confusion matrix on given axis"""
    # Convert from one-hot to class indices if needed
    if targets.ndim == 2:
        targets = np.argmax(targets, axis=1)
    cm = confusion_matrix(targets, y_pred, labels=range(len(CLASSES)))
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    im = ax.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues, vmin=0, vmax=1)
    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    
    ax.set_title("Normalized Confusion Matrix")
    tick_marks = np.arange(len(CLASSES))
    ax.set_xticks(tick_marks)
    ax.set_yticks(tick_marks)
    ax.set_xticklabels(CLASSES, rotation=45, ha="right")
    ax.set_yticklabels(CLASSES)
    
    thresh = cm_normalized.max() / 2.
    for i in range(cm_normalized.shape[0]):
        for j in range(cm_normalized.shape[1]):
            ax.text(j, i, f"{cm_normalized[i, j]:.2f}",
                   ha="center", va="center",
                   color="white" if cm_normalized[i, j] > thresh else "black")
    
    ax.set_ylabel('True label')
    ax.set_xlabel('Predicted label')



    
def plot_combined_results(loader, model, DEVICE, seed=None, best_model_path=None):
    """Main function to generate combined plot
    Args:
        loader: Data loader for the model
        model: Either a model object or a filepath to a saved .pth file
        seed: Optional seed for filename differentiation
    """
    # Handle case where model is a filepath to a .pth file
    if isinstance(model, str) and model.endswith('.pth'):
        # Load the model from state dict
        model_obj = torch.load(model, weights_only = False)
        if isinstance(model_obj, torch.nn.Module):
            # If the .pth file contains the full model
            model = model_obj
        else:
            # If the .pth file contains just the state dict
            # Note: You'll need to know the model architecture to properly load the state dict
            # You might want to add model_class parameter to handle this case
            raise ValueError("State dict loading requires model architecture information")
    
    # Get predictions
    probs, targets = get_model_predictions(loader, model, DEVICE)
    
    # For confusion matrix, we need class predictions
    y_pred = np.argmax(probs, axis=1)
    
    # Create figure
    fig = plt.figure(figsize=(18, 8))
    gs = GridSpec(1, 2, width_ratios=[1, 1])
    
    # Plot PR curves
    ax1 = plt.subplot(gs[0])
    plot_pr_curves(ax1, probs, targets)
    
    # Plot confusion matrix
    ax2 = plt.subplot(gs[1])
    plot_confusion_matrix(ax2, targets, y_pred)
    
    # Save and show
    plt.tight_layout()
    model_name = model.__class__.__name__ if hasattr(model, '__class__') else model.split('/')[-1].replace('.pth', '')
    filename = f'/projects/bcrv/abrown3/jobs/models_img_meta/{model_name}_{seed}_{wandb.run.id}.png' if seed else f'/projects/bcrv/abrown3/jobs/models_img_meta/{model_name}_{wandb.run.id}.png'
    
    wandb.log({"pr, confusion matrix":wandb.Image(fig)})
    #wandb.log({'plot':wandb.Image(plt)
    #    })
    
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show()
    
    
    
    # Print metrics
    pr_auc_mean, pr_aucs = print_metrics(loader, model, DEVICE)
    print(f'\nSaved plots under {filename}\n')
    return pr_auc_mean, pr_aucs, plt
    
    
    
def plot_combined_results(loader, model, DEVICE, seed=None, best_model_path=None):
    """Main function to generate combined plot
    Args:
        loader: Data loader for the model
        model: Either a model object or a filepath to a saved .pth file
        seed: Optional seed for filename differentiation
    """
    # Handle case where model is a filepath to a .pth file
    if isinstance(model, str) and model.endswith('.pth'):
        # Load the model from state dict
        model_obj = torch.load(model, weights_only = False)
        if isinstance(model_obj, torch.nn.Module):
            # If the .pth file contains the full model
            model = model_obj
        else:
            # If the .pth file contains just the state dict
            # Note: You'll need to know the model architecture to properly load the state dict
            # You might want to add model_class parameter to handle this case
            raise ValueError("State dict loading requires model architecture information")
    
    # Get predictions
    probs, targets = get_model_predictions(loader, model, DEVICE)
    
    # For confusion matrix, we need class predictions
    y_pred = np.argmax(probs, axis=1)
    
    # Create figure
    fig = plt.figure(figsize=(18, 8))
    gs = GridSpec(1, 2, width_ratios=[1, 1])
    
    # Plot PR curves
    ax1 = plt.subplot(gs[0])
    plot_pr_curves(ax1, probs, targets)
    
    # Plot confusion matrix
    ax2 = plt.subplot(gs[1])
    plot_confusion_matrix(ax2, targets, y_pred)
    
    # Save and show
    plt.tight_layout()
    model_name = model.__class__.__name__ if hasattr(model, '__class__') else model.split('/')[-1].replace('.pth', '')
    filename = f'/projects/bcrv/abrown3/jobs/models_img_meta/{model_name}_{seed}_{wandb.run.id}.png' if seed else f'/projects/bcrv/abrown3/jobs/models_img_meta/{model_name}_{wandb.run.id}.png'
    
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show()
    
    # Print metrics
    pr_auc_mean, pr_aucs = print_metrics(loader, model, DEVICE)
    print(f'\nSaved plots under {filename}\n')
    return pr_auc_mean, pr_aucs, plt



def calculate_pr_auc_print_metrics(loader, model, device):
    model.eval()
    all_targets = []
    all_probs = []
    
    # For expert load trackings
    fusion_expert_loads = []
    classification_expert_loads = []
    import torch.nn.functional as F

    with torch.no_grad():
        for batch in loader:
            metadata = batch['metadata'].to(device)
            image = batch['image'].to(device)

            target = batch['target'].to(device)
            
            outputs = model(metadata, image=image)
            
            # Store expert weights
            try:
                fusion_expert_loads.append(outputs['fusion_weights'].cpu())
                classification_expert_loads.append(outputs['expert_weights'].cpu())
            except:
                None
            probs = F.softmax(outputs['logits'], dim=1)
            all_targets.append(target.cpu())
            all_probs.append(probs.cpu())
    
    # Calculate PR-AUCs
    all_targets = torch.cat(all_targets).numpy()
    all_probs = torch.cat(all_probs).numpy()


    # class_fractions = np.average(class_counts.numpy()) / class_counts.numpy() 
    
    pr_aucs = []
    weighted_pr_aucs=[]
    for class_idx in range(len(CLASSES)):  # CLASSES = list of class names/indices
        precision, recall, _ = precision_recall_curve(
            all_targets[:, class_idx],  # Binary targets for this class
            all_probs[:, class_idx]     # Predicted probabilities for this class
        )
        pr_auc = auc(recall, precision)
        pr_aucs.append(pr_auc)
        
        # Weight by class fraction
        # weighted_pr_auc = pr_auc / class_fractions[class_idx]
        # weighted_pr_aucs.append(weighted_pr_auc)


    
    # Calculate expert load statistics
    try:
        fusion_loads = torch.cat(fusion_expert_loads).mean(dim=0).numpy()
        classification_loads = torch.cat(classification_expert_loads).mean(dim=0).numpy()
    
        return np.mean(pr_aucs), pr_aucs, fusion_loads, classification_loads
    except:
        return np.mean(pr_aucs), pr_aucs, [0,0,0], [0,0,0]


def print_metrics(loader, model,  DEVICE):
    """Print evaluation metrics"""
    # Print class distribution
    # print("\nTest set class distribution:")
    #class_counts = {i: 0 for i, name in enumerate(CLASSES)}
    #
    #for batch in loader:
    #    targets = batch['target']
    #    if targets.dim() == 2:  # one-hot
    #        batch_indices = torch.argmax(targets, dim=1)
    #    else:  # class indices
    #        batch_indices = targets
    #  
    #  # for idx in range(len(CLASSES)):
    #  #     class_counts[idx] += (batch_indices == idx).sum().item()
    #
    #for idx, name in enumerate(CLASSES):
    #    print(f"{name}: {class_counts[idx]}")

    # PR-AUC
    pr_auc_mean, pr_aucs, fusion_loads, classification_loads = calculate_pr_auc_print_metrics(loader, model, DEVICE)
    print(f'fusion loads:{fusion_loads}')
    # print(f'classification loads: {classification_loads}')
    print(f"\nMean PR-AUC: {pr_auc_mean:.3f}")
    
    for name, auc in zip(CLASSES, pr_aucs):
        print(f"{name}: {auc:.3f}")
    return pr_auc_mean, pr_aucs



def plot_combined_results(loader, model, DEVICE, seed=None, best_model_path=None):
    """Main function to generate combined plot
    Args:
        loader: Data loader for the model
        model: Either a model object or a filepath to a saved .pth file
        seed: Optional seed for filename differentiation
    """
    # Handle case where model is a filepath to a .pth file
    if isinstance(model, str) and model.endswith('.pth'):
        # Load the model from state dict
        model_obj = torch.load(model, weights_only = False)
        if isinstance(model_obj, torch.nn.Module):
            # If the .pth file contains the full model
            model = model_obj
        else:
            # If the .pth file contains just the state dict
            # Note: You'll need to know the model architecture to properly load the state dict
            # You might want to add model_class parameter to handle this case
            raise ValueError("State dict loading requires model architecture information")
    
    # Get predictions
    probs, targets = get_model_predictions(loader, model, DEVICE)
    
    # For confusion matrix, we need class predictions
    y_pred = np.argmax(probs, axis=1)
    
    # Create figure
    fig = plt.figure(figsize=(18, 8))
    gs = GridSpec(1, 2, width_ratios=[1, 1])
    
    # Plot PR curves
    ax1 = plt.subplot(gs[0])
    plot_pr_curves(ax1, probs, targets)
    
    # Plot confusion matrix
    ax2 = plt.subplot(gs[1])
    plot_confusion_matrix(ax2, targets, y_pred)
    
    # Save and show
    plt.tight_layout()
    model_name = model.__class__.__name__ if hasattr(model, '__class__') else model.split('/')[-1].replace('.pth', '')
    filename = f'/projects/bcrv/abrown3/jobs/models_img_meta/{model_name}_{seed}_{wandb.run.id}.png' if seed else f'/projects/bcrv/abrown3/jobs/models_img_meta/{model_name}_{wandb.run.id}.png'
    
    wandb.log({"pr, confusion matrix":wandb.Image(fig)})
    #wandb.log({'plot':wandb.Image(plt)
    #    })
    
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show()
    
    # Print metrics
    pr_auc_mean, pr_aucs = print_metrics(loader, model, DEVICE)
    print(f'\nSaved plots under {filename}\n')
    return pr_auc_mean, pr_aucs, plt



def calculate_pr_auc(loader, model, class_counts, DEVICE):
    model.eval()
    all_targets = []
    all_probs = []
    
    # For expert load trackings
    fusion_expert_loads = []
    classification_expert_loads = []
    import torch.nn.functional as F

    with torch.no_grad():
        for batch in loader:
            metadata = batch['metadata'].to(DEVICE)
            image = batch['image'].to(DEVICE)

            target = batch['target'].to(DEVICE)
            
            outputs = model(metadata, image=image)
            
            # Store expert weights
            try:
                fusion_expert_loads.append(outputs['fusion_weights'].cpu())
                classification_expert_loads.append(outputs['expert_weights'].cpu())
            except:
                None
            probs = F.softmax(outputs['logits'], dim=1)
            all_targets.append(target.cpu())
            all_probs.append(probs.cpu())
    
    # Calculate PR-AUCs
    all_targets = torch.cat(all_targets).numpy()
    all_probs = torch.cat(all_probs).numpy()


    # class_fractions = np.average(class_counts.numpy()) / class_counts.numpy() 
    
    pr_aucs = []
    weighted_pr_aucs=[]
    for class_idx in range(len(CLASSES)):  # CLASSES = list of class names/indices
        precision, recall, _ = precision_recall_curve(
            all_targets[:, class_idx],  # Binary targets for this class
            all_probs[:, class_idx]     # Predicted probabilities for this class
        )
        pr_auc = auc(recall, precision)
        pr_aucs.append(pr_auc)
        
        # Weight by class fraction
        # weighted_pr_auc = pr_auc / class_fractions[class_idx]
        # weighted_pr_aucs.append(weighted_pr_auc)
    
    # Calculate expert load statistics
    try:
        fusion_loads = torch.cat(fusion_expert_loads).mean(dim=0).numpy()
        classification_loads = torch.cat(classification_expert_loads).mean(dim=0).numpy()
    
        return np.mean(pr_aucs), pr_aucs, fusion_loads, classification_loads
    except:
        return np.mean(pr_aucs), pr_aucs, [0,0,0], [0,0,0]



    
    
    
def calculate_val_loss(loader, model, criterion, DEVICE):
    """Calculate validation loss using the same criterion as training.
    
    Args:
        loader: DataLoader for validation data
        model: Model to evaluate
        criterion: Loss function (same as used in training)
        DEVICE: Device to run calculations on
    
    Returns:
        float: Average validation loss
    """
    model.eval()
    val_loss = 0.0
    
    with torch.no_grad():
        for batch in loader:
            metadata = batch['metadata'].to(DEVICE)
            image = batch['image'].to(DEVICE)
            target = batch['target'].to(DEVICE)
            
            outputs = model(metadata, image)
            loss = criterion(outputs['logits'], target)
            val_loss += loss.item()
    
    return val_loss / len(loader)


class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-6):
        super().__init__()
        self.smooth = smooth  # Prevents division by zero

    def forward(self, y_pred, y_true):
        # Convert y_true to one-hot if needed (assuming y_true is class indices)
        if y_true.dim() == 1:
            y_true = F.one_hot(y_true, num_classes=y_pred.size(1)).float()
        
        # Softmax for multi-class
        y_pred = F.softmax(y_pred, dim=1)
        
        # Compute Dice per class
        intersection = (y_pred * y_true).sum(dim=0)
        union = y_pred.sum(dim=0) + y_true.sum(dim=0)
        dice = (2. * intersection + self.smooth) / (union + self.smooth)
        
        # Average across classes (you can also weight classes here)
        return 1 - dice.mean()
    
class FocalLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
        # Ensure alpha is properly formatted
        if alpha is not None:
            self.alpha = alpha

    def forward(self, inputs, targets):
        # Convert one-hot targets to class indices if needed
        if targets.dim() > 1 and targets.size(1) > 1:
            targets = targets.argmax(dim=1)
        
        # Ensure targets are long integers
        targets = targets.long()
        
        logpt = F.log_softmax(inputs, dim=1)
        logpt = logpt.gather(1, targets.unsqueeze(1))
        logpt = logpt.view(-1)
        pt = logpt.exp()

        loss = -((1 - pt) ** self.gamma) * logpt

        if self.alpha is not None:
            alpha = self.alpha.to(targets.device)
            at = alpha.gather(0, targets)
            loss = loss * at

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss


class FocalLossWithExpertSpecialization(nn.Module):
    """
    Focal loss with auxiliary loss that encourages class-specific expert specialization.
    """
    def __init__(self, alpha=None, gamma=2, expert_weight=0.1):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.expert_weight = expert_weight  # Weight for the auxiliary expert loss
        self.ce_loss = nn.CrossEntropyLoss(weight=alpha, reduction='none')

    def forward(self, inputs, targets, expert_weights=None):
        """
        Args:
            inputs: logits from model [batch_size, num_classes]
            targets: one-hot encoded targets [batch_size, num_classes]
            expert_weights: expert weights from model [batch_size, num_experts]
        Returns:
            Combined focal loss + expert specialization loss
        """
        # Convert one-hot to class indices if needed
        if targets.dim() == 2:
            class_indices = torch.argmax(targets, dim=1)
        else:
            class_indices = targets
        
        # Calculate standard focal loss
        ce_loss = self.ce_loss(inputs, class_indices)
        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()
        
        if expert_weights is not None and self.expert_weight > 0:
            # Auxiliary loss: encourage each class to use its corresponding expert
            batch_size = expert_weights.shape[0]
            num_experts = expert_weights.shape[1]
            
            # Create ideal expert weights (one-hot for corresponding expert)
            ideal_expert_weights = F.one_hot(
                class_indices % num_experts,  # Cycle through experts if more classes than experts
                num_classes=num_experts
            ).float()
            
            # Calculate expert specialization loss (MSE between actual and ideal weights)
            expert_loss = F.mse_loss(expert_weights, ideal_expert_weights)
            
            # Combine losses
            total_loss = focal_loss + self.expert_weight * expert_loss
        else:
            total_loss = focal_loss
            
        return total_loss



        
class MultiClassBCELoss(nn.Module):
    def __init__(self, ignore_index=-100, epsilon=1e-8):
        super().__init__()
        self.ignore_index = ignore_index
        self.epsilon = epsilon

    def forward(self, inputs, targets):
        # Ensure proper dimensions and dtype
        targets = targets.long()
        
        if inputs.dim() == 1:
            # Handle binary classification case
            inputs = inputs.unsqueeze(1)
            inputs = torch.cat([1-inputs, inputs], dim=1)
        
        # Get probabilities
        probs = F.softmax(inputs, dim=1)
        batch_size, num_classes = probs.shape
        
        # Create correct class mask
        correct_mask = torch.zeros_like(probs)
        if targets.dim() == 1:
            # Standard class indices case
            correct_mask.scatter_(1, targets.unsqueeze(1), 1.0)
        else:
            # Handle one-hot case
            correct_mask = targets.float()
        
        # Calculate loss components
        correct_term = (probs * correct_mask).sum(dim=1) / num_classes
        incorrect_terms = ((1 - probs) * (1 - correct_mask)).sum(dim=1) / num_classes
        
        # Combine terms
        loss_per_sample = -(correct_term + incorrect_terms)
        
        return loss_per_sample.mean()

    
def get_class_counts(dataloader):
    num_classes = len(CLASSES)  # Always use coarse classes
    counts = torch.zeros(num_classes, dtype=torch.long)
    
    for batch in dataloader:
        targets = batch['target']
        # For one-hot encoded targets (assuming your targets are one-hot)
        if targets.dim() == 2:
            counts += targets.sum(dim=0).long()
        # For class index targets (alternative format)
        else:
            class_indices = targets.argmax(dim=1) if targets.dim() == 2 else targets
            counts += torch.bincount(class_indices, minlength=num_classes)
    # print(counts)
    
    return counts


    
    
    
def main(config, trial):
    tnow = t.now()

    
    config = get_config(trial)
    
    
    #config_path='/projects/bcrv/abrown3/XastroMiNN_50_epoch.json'
    #with open(config_path,'r') as f:
    #    config = json.load(f)

    BATCH_SIZE = config['batch_size']
    # LR = 0.0006777718906668259  
    LR = config['learning_rate']
    EPOCHS = config['epochs']
    PATIENCE =  config['patience']
    
    print("config!\n", config, "\n")

    run = wandb.init(project="CiDEr_image_meta",
                     notes="trials.... ", config=config, group=STUDY_NAME, 
                     reinit=True,
                     tags=['trial', '7-5'])
    
    for run in range(int(1)):
        print("Run ID:", wandb.run.id)
        
        # Log config file from local file
        #artifact_config = wandb.Artifact(name="config-file", type="file")
        #artifact_config.add_file(local_path=config_path, name=f'{wandb.run.id}-config.json')
        #wandb.log_artifact(artifact_config)
        wandb.log({"Model": 'XastroMiNN'})

        h = random.randint(100, 190)
        # loader_seed = h + (run*9)
        
        seed = config['seed']
        loader_seed=config['loader_seed']
        # print(f'using loader seed:{loader_seed}')
        # Python and numpy
        random.seed(seed)
        np.random.seed(seed)
        
        # PyTorch
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
        
        # Configure PyTorch for deterministic behavior
        torch.backends.cudnn.deterministic = True  # This makes CUDA operations deterministic
        torch.backends.cudnn.benchmark = False     # Should be False for reproducibility
        
        # Set Python hash seed (I am paranoid)
        os.environ['PYTHONHASHSEED'] = str(seed)
        
        #============================================================
        #    Initialize model and optimize tower parameters
        # (this is where it's a bit like taming a pack of dragons)
        #============================================================
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # add
        
        model = XastroMiNN(num_classes=len(CLASSES),num_mlp_experts=config['num_experts'],
                    towers_hidden_dims = config['towers_hidden_dims'],
                    towers_outdims = config['towers_outdims'],
                    fusion_hidden_dims = config['fusion_hidden_dims'],
                    fusion_router_dims = config['fusion_router_dims'],
                    fusion_outdims = config['fusion_outdims']
                    ).to(device)

        if model.__class__.__name__ == 'XastroMiNN':
            optimizer = torch.optim.AdamW([
                    # Image/Coord towers - moderate regularization
                    {'params': model.image_tower.parameters(), 'weight_decay': config['cnn_decay'], 'lr': LR*config['cnn_lr']},
                    
                    # Metadata towers - higher regularization (more prone to overfitting)
                    {'params': model.psf_tower.parameters(), 'weight_decay': config['psf_decay'], 'lr': LR*config['psf_lr']},  
                    {'params': model.lc_tower.parameters(), 'weight_decay': config['lc_decay'], 'lr': LR*config['lc_lr']},  
                    {'params': model.mag_tower.parameters(), 'weight_decay': config['mag_decay'], 'lr': LR*config['mag_lr']},  
                    {'params': model.spatial_tower.parameters(), 'weight_decay': config['spatial_decay'], 'lr': LR*config['spatial_lr']},
                    {'params': model.coord_tower.parameters(), 'weight_decay': config['nst1_decay'], 'lr': LR*config['nst1_lr']},
                    {'params': model.nst1_tower.parameters(), 'weight_decay': config['nst1_decay'], 'lr': LR*config['nst1_lr']},
                    {'params': model.nst2_tower.parameters(), 'weight_decay': config['nst2_decay'], 'lr': LR*config['nst2_lr']},

                    # Currently using mega tower as the router essentially
                    {'params': model.mega_tower.parameters(), 'weight_decay': config['lc_decay'], 'lr': LR*config['lc_lr']},

                    # Fusion components
                    {'params': model.fusion_experts.parameters(), 
                    'weight_decay': config['fusion_decay'],  # Reduced from 5e-4
                    'lr': LR*config['fusion_lr'],
                    'betas':(config['fusion_beta1'], config['fusion_beta2'])
                    },  
                    
                    {'params': model.fusion_router.parameters(), 
                    'weight_decay': config['router_decay'],
                    'lr':LR*config['router_lr'],
                    'betas':(config['router_beta1'], config['router_beta2'])
                    }

                    ], lr=LR, 
                    # betas=(0.9205528268203966, 0.9703153101672825),
                    betas=(config['beta1'], config['beta2']), 
                    eps=config['eps'])  # Avoids divison by zero errors         
            print(f'using {model.__class__.__name__} model with test optimizing parameters')
        else:
            optimizer = torch.optim.AdamW(params=model.parameters(), lr=LR, betas=(config['beta1'], config['beta2']), eps=config['eps'])


        if config['scheduler'] == 'cosine_annealing':
            scheduler = CosineAnnealingLR(optimizer, config['t_max'])
        if config['scheduler'] == 'reduce_on_plateau':
            scheduler = ReduceLROnPlateau(optimizer, 'min',min_lr=config['min_lr'], patience=config['sched_pat'], factor=config['sched_factor'])
        #parent_dir = str(Path(__file__).parent)
        
        wandb.log({
           "gpu":config['gpu'],
            "embed_freq":config["embed_freq"],
            "learning_rate":config["learning_rate"],  # 5e-4,
            "epochs":config["epochs"],
            "patience":config["patience"],
            "batch_size":config["batch_size"],
            "seed":config["seed"],
            "loader_seed":config["loader_seed"],
            "num_experts":config["num_experts"],
            "towers_hidden_dims":config["towers_hidden_dims"],
            "towers_outdims":config["towers_outdims"],
            "embedding":config["embedding"],
            
            
            "dataset classes":config["dataset classes"],
            "classes": config["classes"],
            
            "fusion_hidden_dims":config["fusion_hidden_dims"],
            "fusion_router_dims":config["fusion_router_dims"],
            "fusion_outdims":config["seed"],
            
            "cnn_lr":config["cnn_lr"],
            "cnn_decay":config["cnn_decay"],
            "psf_lr":config["psf_lr"],
            "psf_decay":config["psf_decay"],
            "mag_lr": config["mag_lr"],
            "mag_decay": config["mag_decay"],
            "lc_lr": config["lc_lr"],
            "lc_decay":config["lc_decay"],
            "spatial_lr":config["spatial_lr"],
            "spatial_decay":config["spatial_decay"],
            "coord_lr": config["coord_lr"],
            "coord_decay": config["coord_decay"],
            
            "nst1_lr":config["nst1_lr"],
            "nst1_decay":config["nst1_decay"],
            "nst2_lr":config["nst2_lr"],
            "nst2_decay":config["nst2_decay"],
            
            "fusion_lr":config["fusion_lr"],
            "fusion_decay":config["fusion_decay"],
            "fusion_beta1":config["fusion_beta1"],
            "fusion_beta2":config["fusion_beta2"],
            
            "router_decay":config["router_decay"],
            "router_lr":config['router_lr'],
            "router_beta1":config["router_decay"],
            "router_beta2":config["router_decay"],
            "router_lr_2":config['router_lr_2'],
            "router_beta1_2":config["router_beta1_2"],
            "router_beta2_2":config["router_beta2_2"],
            
            "classifier_decay":config['classifier_decay'],
            "classifier_lr":config['classifier_lr'],
            "classifier_beta1":config["classifier_beta1"],
            "classifier_beta2":config["classifier_beta2"],
            
            "beta1":config["beta1"],
            "beta2":config["beta2"],
            "eps":config["eps"],
            "sched_pat":config["sched_pat"],
            "sched_factor":config["sched_pat"],
            "min_lr":config["min_lr"],
            "weight_exp":config["weight_exp"],
            
            "gamma":config["gamma"],
            "criterion": config["criterion"],
            "scheduler": config["scheduler"], 
            
            "t_max": config['t_max'],
            "max_norm":config['max_norm']})
        
        
        
        # Initialize data loaders'
        print("Loading data...")
        train_loader = get_data_loader(npy_dir='/work/nvme/bcrv/abrown3/preprocessed_data/data_multi/day10/',
                              alert_file_list=Train['file'].to_list(), batch_size=32, random_alert_per_epoch=False)
                              # alert_file_list=custom_train_list, batch_size=32, random_alert_per_epoch=False)
        
        val_loader = get_data_loader(npy_dir='/work/nvme/bcrv/abrown3/preprocessed_data/data_multi/day10/',
                                    #alert_file_list=custom_val_list, batch_size=32, random_alert_per_epoch=False)
                                     alert_file_list=Val['file'].to_list(), batch_size=32, random_alert_per_epoch=False)
        
        test_loader = get_data_loader(npy_dir='/work/nvme/bcrv/abrown3/preprocessed_data/data_multi/day10/',
                              #alert_file_list=custom_test_list, batch_size=32, random_alert_per_epoch=False)
                              alert_file_list=Test['file'].to_list(), batch_size=32, random_alert_per_epoch=False)
        
        print("Finished Loading Data.")

        # assign class weights 
        class_counts = get_class_counts(train_loader)
        print("class_counts:", class_counts)
        
        weight_exponent = config['weight_exp']
        if weight_exponent==1:
            numberator = 30000
        else:
            numberator=5000
        coarse_weights = torch.tensor([
                numberator/(int(count)**(weight_exponent))                  
                for idx, count in enumerate(class_counts)
            ], device=device, dtype=torch.float32)

        # Initialize criterion with the class weights
        if config['criterion'].lower() == 'focalexpert':

            criterion = FocalLossWithExpertSpecialization(
            #criterion = lc.FocalLossWithExpertSpecialization(
                alpha=coarse_weights, 
                gamma=config['gamma'],
                expert_weight=.5  # Add this to your config
            )

        if config['criterion'].lower() == 'focal':
            criterion = FocalLoss(alpha=coarse_weights, gamma=config['gamma'])

        if config['criterion'].lower() == 'binary':
            criterion = nn.BCELoss(weight=coarse_weights)
        if config['criterion'].lower() == 'cross_entropy':
            criterion = nn.CrossEntropyLoss(weight=coarse_weights)

        # else:
        #     print('selected criterion not found in supported list: "focal", "binary", "cross_entropy"')
        # criterion = lc.MultiClassBCELoss()

        #============================================================
        # Main Training Loop
        #============================================================

        best_pr_auc = 0
        best_val_loss = 10
        epochs_no_improve = 0
        # small_batch = config['']
        
        for epoch in range(EPOCHS):
            model.train()
            train_loss = 0.0

            for batch in tqdm(train_loader,total=len(train_loader), desc='Training', leave=False):
                #print("batch!!: ", batch, "\n")
                
                metadata = batch['metadata'].to(device)
                image = batch['image'].to(device)
                target = batch['target'].to(device)
                #print("image:\n", image,"\n" )
                #print("meta:\n", metadata )

                optimizer.zero_grad()

                outputs = model(metadata, image=image)
                
                #print("outputs.shape", outputs.shape)
                
                if config['criterion'].lower() == 'focalexpert':
                    loss = criterion(outputs['logits'], target, outputs['fusion_weights'])
                else:
                    loss = criterion(outputs['logits'], target)

                loss.backward()

                nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['max_norm'])
                optimizer.step()
                train_loss += loss.item()

        
            val_pr_auc_mean, val_pr_aucs, _, _ = calculate_pr_auc(val_loader, model, class_counts,  device)
            # val_loss = calculate_val_loss(val_loader, model, criterion, DEVICE)

            train_loss /= len(train_loader)

            if config['scheduler'] == 'cosine_annealing':
                scheduler.step()
            if config['scheduler'] == 'reduce_on_plateau':
                # scheduler.step(val_loss)
                scheduler.step(1-val_pr_auc_mean)

            if val_pr_auc_mean > best_pr_auc:
            # if best_val_loss > val_loss:
                print(val_pr_auc_mean)
                best_pr_auc = val_pr_auc_mean
                # best_val_loss=val_loss
                epochs_no_improve = 0
                torch.save(model.state_dict(), f'/projects/bcrv/abrown3/jobs/models_img_meta/best_model{seed}_{wandb.run.id}.pth')
                model_best = copy.deepcopy(model) # there's definitely a better way to do this but I don't wanna look it up
                
            else:
                epochs_no_improve += 1
                if epochs_no_improve == PATIENCE:
                    print(f"Early stopping at epoch {epoch+1}")
                    
                    wandb.log({
                        "early stop at":{epoch+1},
                    })        
                
                    break
            
 
                    
            wandb.log({
                "epoch":epoch,
                "train_loss": train_loss,
                # "val_loss": val_loss,
                "val_auprc_mean": val_pr_auc_mean,
                **{f"val_pr_auc_{name}": auc for name, auc in zip(SHOW_CLASSES, val_pr_aucs)},
                "best_val_loss": best_val_loss,
                "learning_rate": optimizer.param_groups[0]['lr'],
                "best_val_auprc": best_pr_auc
            })
            
            pr_auc_str = "|".join([f"{name}:{auc:.3f}" for name, auc in zip(SHOW_CLASSES, val_pr_aucs)])
            print(f"Epoch {epoch+1}/{EPOCHS}|"
                f"Train Loss:{train_loss:.4f}|"
                
                # f"Val loss:{val_loss:.3f}|"
                
                  f"Macro mean AUPRC:{val_pr_auc_mean:.4f}|"
                f"Class AUPRCs:{pr_auc_str}")
        print(f'best loss{best_val_loss}')
        # Evaluation
        random_stats = random_baseline_pr_auc(test_loader, n_trials=1000)
        print(f"Random Baseline PR-AUCs (mean ± std):")
        for i, class_name in enumerate(CLASSES):
            print(f"{class_name}: {random_stats['mean'][i]:.3f} ± {random_stats['std'][i]:.3f}")
        
        # Plot and save results
        pr_auc_mean, pr_aucs, plt = plot_combined_results(test_loader, model_best, device)
        
        
        
        wandb.log({
            #'confusion_matrix': wandb.Image(plt), # saving plots after plt.show() saves blank
            'test_auprc_mean': pr_auc_mean,
             **{f"test_auprc_{name}": auc for name, auc in zip(SHOW_CLASSES, pr_aucs)}
        })
        
        wandb.save(f'models/best_model_{seed}_{wandb.run.id}.pth')
        print(f'Time taken so far: {t.now()-tnow}')
        print("Run ID:", wandb.run.id)
        wandb.finish()

# NO! 
#def parse_args():
#    parser = argparse.ArgumentParser(description='select a model')
#    parser.add_argument('runs', type=str, nargs='?', default='XastroMiNN',
#                      help='which model do you want to use? (default: XastroMiNN)')
#    return parser.parse_args().runs
def random_baseline_pr_auc(loader, n_trials=1000):
    all_targets = []
    for batch in loader:
        targets = batch['target']  # Directly use the target tensor

        # Convert one-hot to class indices if needed
        if targets.dim() == 2:
            targets = torch.argmax(targets, dim=1)

        all_targets.append(targets.cpu().numpy())
    targets = np.concatenate(all_targets)
    
    num_classes = len(CLASSES) 
    trial_pr_aucs = np.zeros((n_trials, num_classes))

    
    for trial in range(n_trials):
        np.random.seed(trial)
        # Generate random probabilities that sum to 1
        random_probs = np.random.dirichlet(np.ones(num_classes), size=len(targets))
        
        for class_idx in range(num_classes):
            precision, recall, _ = precision_recall_curve(
                (targets == class_idx).astype(int),
                random_probs[:, class_idx]
            )
            trial_pr_aucs[trial, class_idx] = sklearn_auc(recall, precision)
    
    return {
        'mean': np.mean(trial_pr_aucs, axis=0),
        'std': np.std(trial_pr_aucs, axis=0),
        'all_trials': trial_pr_aucs
    }

#if __name__ == "__main__":
#    main(config, trial)
    
    
    
def get_config(trial):
    
    config = {
        
        "mode": 'trial',
        "fusion trial": True,
        "classifier trial": True,
        "basic trial":True,
        
        "dataset classes":[['SN Ia','SN Ic','SN Ib'],[ 'SN IIP', 'SN IIn','SN II'], ['Cataclysmic'], ['AGN'], ['Tidal Disruption Event']],
        "classes": ['SN I', 'SN II', 'Cataclysmic', 'AGN','Tidal Disruption Event'],

        
        
        "gpu":1,
        "embed_freq": 7,
        
        "learning_rate":1.6e-4, 
        
        #"epochs":50,
        "patience":3,
        "batch_size":128,
        "seed":135,
        "loader_seed":125,
        
        "num_experts":4,
        "towers_hidden_dims":16,
        "towers_outdims":4,
        "embedding":"False",
        
        "fusion_hidden_dims":128,
        "fusion_router_dims":128,
        "fusion_outdims":4,
        
        "cnn_lr":2,
        "cnn_decay":5e-2,
        "psf_lr":0.5,
        "psf_decay":5e-2,
        "mag_lr": 2,
        "mag_decay": 0.0,
        
        "lc_lr": 2,
        "lc_decay": 0.05,
        "spatial_lr":2,
        "spatial_decay":0.0,
        
        "coord_lr": 0.5,
        "coord_decay": 0.0,
        
        "nst1_lr":2,
        "nst1_decay":0.0,
        "nst2_lr":2,
        "nst2_decay":0.0,
        
        "fusion_lr":1,
        "fusion_decay":1e-2,
        "fusion_beta1": 0.9,
        "fusion_beta2": 0.999,
        
        "router_decay":0.0,
        "router_lr":1.5,
        "router_beta1":0.9,
        "router_beta2":0.999,
        "router_lr_2":1,
        "router_beta1_2":0.95,
        "router_beta2_2":0.99,
        
        "classifier_decay": 0.0,
        "classifier_lr":2.44,
        "classifier_beta1":0.95,
        "classifier_beta2":0.99,
        
        "beta1":0.9,
        "beta2":0.999,
        "eps":5e-10,
        
        "sched_pat":5,
        "sched_factor":0.4,
        
        "min_lr":6e-10,
        
        "weight_exp":1,
        "gamma":2.5,
        
        "criterion": "cross_entropy",
        "scheduler": "cosine_annealing", 
        "t_max": 6,
        "max_norm":5
}

    if STUDY_NAME.startswith('img_meta'):
        config['epochs'] = 100

    else:
        raise NotImplementedError(f"Unknown study name {STUDY_NAME}")

    if config["basic trial"]:
        config['learning_rate'] = trial.suggest_float('learning_rate', 1e-5, 1e-2)
        config['beta1'] = trial.suggest_float('beta1', 0.7, 0.95)
        config['beta2'] = trial.suggest_float('beta2', 0.7, 0.95)
        
    #"fusion trial": True,
    #"classifier trial": True,
    
    if config["classifier trial"]:
        config['classifier_decay'] = trial.suggest_float('classifier_decay',  1e-5, 1e-2)
        config['classifier_lr'] = trial.suggest_float('classifier_lr', 1.6, 2.6)
        config['classifer_beta1'] = trial.suggest_float('classifer_beta1', 0.75, 0.99)
        config['classifer_beta2'] = trial.suggest_float('classifer_beta2',  0.75, 0.99)
        
    if config["fusion trial"]:
        config['fusion_lr'] = trial.suggest_float('fusion_lr', 1e-5, 1e-2, log=True)
        config['fusion_decay'] = trial.suggest_float('fusion_decay', 1e-6, 1e-3)
        config['fusion_beta1'] = trial.suggest_float('fusion_beta1', 0.80, 0.9999)
        config['fusion_beta2'] = trial.suggest_float('fusion_beta2',  0.80, 0.9999)
        
    config['study_name'] = STUDY_NAME

    return config


class ResidualTowerBlock(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.start_path = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.GELU()
        )

        self.main_path = nn.Sequential(
            nn.LayerNorm(hidden_dim),
            nn.Dropout(.25),
            nn.Linear(hidden_dim, output_dim))

        self.activation = nn.Sequential(
            nn.LayerNorm(hidden_dim),
            nn.Dropout(.25),
            nn.Linear(hidden_dim, output_dim),
            nn.Sigmoid()
        )


        self.skip_path = nn.Linear(input_dim, output_dim) if input_dim != output_dim else nn.Identity()
        
    def forward(self, x):
        first_half = self.start_path(x)
        gating = self.activation(first_half)
        out = self.main_path(first_half)*gating  + self.skip_path(x)

        return out
    
    

# 🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏 REAL MODEL 🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏🍏


device = torch.device("cuda")

class XastroMiNN(nn.Module):
    """
    Image and Metadata transient classifier
    """
    
    def __init__(self, num_classes=5, num_mlp_experts=4, towers_hidden_dims = 16,
                 towers_outdims = 32,
                 fusion_hidden_dims = 128,
                 fusion_router_dims = 128,
                 fusion_outdims = 32, config=None
                 ):
        super().__init__()
        
        #self.classification = True if config['mode'] == 'metdata & images' else False
        
        self.has_image = True  # Flag for image availability
        self.num_classes = num_classes
        self.towers_hidden_dims = towers_hidden_dims
        self.towers_outdims = towers_outdims

        self.fusion_hidden_dims = fusion_hidden_dims  # was 1024
        self.fusion_router_dims = fusion_router_dims # was 256
        self.fusion_outdims = fusion_outdims


        # ===== Metadata Processing Towers ===== 
        # Each tower processes specific metadata features
        # PSF quality features tower 
        self.psf_tower = ResidualTowerBlock(2, self.towers_hidden_dims, towers_outdims)
    
        # Magnitude features tower 
        self.mag_tower = ResidualTowerBlock(7, self.towers_hidden_dims*2, towers_outdims)
        
        # LC features tower
        self.lc_tower = ResidualTowerBlock(12, self.towers_hidden_dims*3, towers_outdims)

        # Spatial features tower (distpsnr1, distpsnr2, nmtchps)
        self.spatial_tower = ResidualTowerBlock(3, self.towers_hidden_dims, towers_outdims)

        # Nearest source features tower 1 (sgscore1, distpsnr1)
        self.nst1_tower = ResidualTowerBlock(2, self.towers_hidden_dims, fusion_outdims)
        # self.mega_tower = ResidualTowerBlock(7, 64, self.towers_outdims)
        # Nearest source features tower 2 (sgscore2, distpsnr2)
        self.nst2_tower = ResidualTowerBlock(2, self.towers_hidden_dims, fusion_outdims)

        self.coord_tower = ResidualTowerBlock(2, self.towers_hidden_dims, fusion_outdims)

        self.mega_tower = ResidualTowerBlock(19, 128, towers_outdims)

        # ===== Image Processing =====
        # self.image_tower = timm.create_model(
        #     'vit_tiny_patch16_224',
        #     pretrained=True,
        #     num_classes=32,
        #     img_size=49,
        #     in_chans=4
        # )
        
        # 🤡🤡🤡 add .to(device)
        self.image_tower = SplitHeadConvNeXt(
                    pretrained=False,       # or False if training from scratch
                    in_chans=4,             # Critical: override default 3-channel input
                    outdims=towers_outdims  # Your task's number of classes
                ).to(device)

        fusion_dims = 6*towers_outdims + 3*fusion_outdims 
        # ===== Modality Fusion MoE ===== 
        # Combines features from all towers (4 metadata + image)
        self.fusion_experts = nn.ModuleList([
            ResidualTowerBlock(fusion_dims, fusion_hidden_dims, 5)
            for _ in range(num_mlp_experts)
        ])

        num_experts=num_mlp_experts 
        self.fusion_router = nn.Sequential(
            nn.Linear( fusion_dims, fusion_dims//2),
            nn.Tanh(),
            nn.Dropout(0.3),
            nn.Linear(fusion_dims//2, num_experts),
            # nn.Softmax(dim=-1)  # each expert gets independent [0,1] weight
            nn.Sigmoid()
        )

    def forward(self, metadata, image):
        '''    Processes input metadata and optional image data through specialized towers,
            combines features using a Mixture of Experts (MoE) approach, and returns
            classification logits with expert weighting information.

            Parameters:
            -----------
            metadata : torch.Tensor
                Input metadata tensor of shape (batch_size, num_metadata_features).
                Expected to contain 24 metadata features (indices 0-23).
            image : torch.Tensor, optional
                Input image tensor of shape (batch_size, channels, height, width).
                If None, image features will be zero-initialized.

            Returns:
            --------
            dict
                Dictionary containing:
                - 'logits': torch.Tensor
                    Output classification logits of shape (batch_size, num_classes)
                - 'expert_weights': torch.Tensor
                    Raw fusion weights for all experts
                - 'fusion_weights': torch.Tensor
                    Same as expert_weights (maintained for compatibility)'''

        # Process all metadata features through respective towers
        psf_feats = self.psf_tower(metadata[:, [5,14]])  # PSF features
        lc_feats = self.lc_tower(metadata[:, [6, 9, 10, 13, 15, 17, 18, 19, 20, 21, 22, 23]])
        mag_feats = self.mag_tower(metadata[:, [6, 9, 10, 13, 15, 17, 18]]) 

        spatial_feats = self.spatial_tower(metadata[:, [2,3,4]])  # Spatial features
        nsta = self.nst1_tower(metadata[:, [0,2]])  # Nearest source A features
        nstb = self.nst2_tower(metadata[:, [1,3]])  # Nearest source B features
        coord_feats = self.coord_tower(metadata[:, [7,8]])
        megatower = self.mega_tower(metadata[:, [0,1,2,3,4,5,6,7,8,9,10,11,12, 13, 14,15, 16, 17, 18]])
        
        # Process image if available (zeros otherwise)
        image_feats = self.image_tower(image) if image is not None else torch.zeros_like(nsta)

        # Concatenate all features for fusion
        all_feats = torch.cat([nsta, nstb, spatial_feats, psf_feats, mag_feats, coord_feats, megatower, image_feats, lc_feats], dim=1)

        # Fusion MoE - combine features from all modalities
        fusion_weights = self.fusion_router(all_feats)

        moe_output = torch.zeros(metadata.size(0), 5, device='cuda') # ,device=metadata.to(device))

        
        topk_weights, topk_indices = torch.topk(fusion_weights, k=2, dim=-1)  # [B, k]

        # Process only through selected experts
        for expert_idx, expert in enumerate(self.fusion_experts):
            # Mask for samples where this expert is in top-k
            expert_mask = (topk_indices == expert_idx).any(dim=-1)  # [B]   # 'ResidualTowerBlock'
            
            if expert_mask.any():
                # Get weights for this expert [M] where M=sum(expert_mask)
                weights = topk_weights[expert_mask, (topk_indices[expert_mask] == expert_idx).nonzero()[:, 1]]
                # Compute expert output only for relevant samples
                expert_out = expert(all_feats[expert_mask])  # [M, num_classes]
                # Weighted contribution
                moe_output[expert_mask] += weights.unsqueeze(-1) * expert_out

        return moe_output